{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1 OWCA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqNEBXW452O8teO39Y+jn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jufabeck2202/KI-Lab/blob/main/%5BKI-Lab2021%5D%20Assignment%201%20OWCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B76tv_iQSm4"
      },
      "source": [
        "# Assignment:\n",
        "Preparation:\n",
        "1. Checkout the notebook 0_Simple_NN.ipynb from Git, which describes how to\n",
        "setup a simple feedforward network on some fake data. Try to understand\n",
        "everything and execute the notebook. Ask on the chat if sth. is unclear.\n",
        "2. Read through this blog post: https://nextjournal.com/gkoehler/pytorch-mnist\n",
        "It shows how to work with data loaders, how to load the MNIST dataset and how\n",
        "training is done in batches (using data loaders). \n",
        "\n",
        "Task:\n",
        "1. Load the MNIST dataset into train and test data loaders. Use the same\n",
        "parameters and apply the same transformations like described in the blog post.\n",
        "2. Create a feedforward neural network consisting of an input layer, one hidden\n",
        "layer of size 100 and an output layer (same structure as in Simple_NN.ipynb).\n",
        "For training on the MNIST dataset you need to change the following:\n",
        "  - Adjust the size of the input layer to be able to take in the MNIST data (hint: you\n",
        "must adjust the tensor format from the MNIST data into a flat structure).\n",
        "  - Use log_softmax as activation function for the output layer (as in the blog).\n",
        "Note: Do not use a CNN like they do in the blog post! Use Relu as activation\n",
        "function for the hidden layer.\n",
        "3. Train your network on the training data for 50 epochs using the negative log\n",
        "likelihood loss (like in the blog). Create a plot of the training loss (like in the blog\n",
        "but without the test loss).\n",
        "4. Test the network on the MNIST test data and give out accuracy and loss.\n",
        "5. Find out how the model can be trained on the GPU instead of the CPU. Compare the\n",
        "training time between CPU and GPU. (Note: Do not except too much improvement on\n",
        "this small data set).\n",
        "6. Repeat step 1. to 4. and see what happens when you try out:\n",
        "  1. Two different hidden layer sizes.\n",
        "  2. Using a normal softmax as activation function together with a CrossEntropyLoss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaBjs1UJL0i_"
      },
      "source": [
        "#Import Libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-hQnZVSdZ-o"
      },
      "source": [
        "# Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i03xd-ldXSJ",
        "outputId": "25342371-2807-439a-be9b-7e5d5d35049e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_epochs = 2\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 1\n",
        "GPU_ON = True\n",
        "torch.backends.cudnn.enabled = False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(random_seed)\n",
        "print(device)\n",
        "\n",
        "GPU_time = 0\n",
        "CPU_time = 0\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE1UpJkCN5cE"
      },
      "source": [
        "# Download MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HK7bLraL0jD"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWnfW6dEd7zE"
      },
      "source": [
        "# Explore Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7wEVruJeClG"
      },
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uoTnlA7eLRy",
        "outputId": "fb518346-115d-4b14-ed7b-a1f698276585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_data.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5J53sWleRxU",
        "outputId": "4f22fd58-1d72-412b-f88b-c4aaf2d2f3e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g84REZy4O8-j"
      },
      "source": [
        "# Create Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCiyk149O_Xo"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(28*28*1, 100)\n",
        "        self.output = torch.nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        x = x.flatten(start_dim = 1) \n",
        "        x = self.hidden(x).clamp(min=0)\n",
        "        x = F.log_softmax(self.output(x))\n",
        "        return x"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFnQZK6Lehgv"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXkPa2pcHLBe"
      },
      "source": [
        "network = Net()\n",
        "if GPU_ON:\n",
        "  network.cuda()\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmvYBcPYOq1y"
      },
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      if GPU_ON:\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "      output = network(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY6zYMMv0mc5"
      },
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    if GPU_ON:\n",
        "      data = data.cuda()\n",
        "      target = target.cuda()\n",
        "    output = network(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beJ0heCwOs1R",
        "outputId": "f0dc551a-d166-4b3c-eb1b-16e054c09c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "#CPU:\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "\n",
        "GPU_ON = False\n",
        "CPU_time = time.time()\n",
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()\n",
        "\n",
        "CPU_time = time.time() - CPU_time\n",
        "\n",
        "#GPU\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "\n",
        "GPU_time = time.time()\n",
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()\n",
        "\n",
        "GPU_time = time.time() - GPU_time\n",
        "\n",
        "print(\"--- CPU: %s seconds ---\" % (CPU_time))\n",
        "print(\"--- GPU: %s seconds ---\" % (GPU_time))\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.3206, Accuracy: 1176/10000 (12%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.384041\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.051963\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.782948\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.337705\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.229805\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.091962\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.099196\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.857733\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.689224\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.700881\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.610868\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.715553\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.632935\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.448870\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.397943\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.508472\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.484946\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.349393\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.472946\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.513482\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.441977\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.463477\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.370201\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.451279\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.490604\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.389210\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.524778\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.455957\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.456382\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.397887\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.173805\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.313133\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.522313\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.346738\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.274435\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.305232\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.330815\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.371660\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.293644\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.210126\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.287948\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.307397\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.278972\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.372863\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.197717\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.463100\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.292554\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.353399\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.288879\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.400115\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.302360\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.464809\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.247089\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.365299\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.346804\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.268476\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.469041\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.265105\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.505060\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.304847\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.208114\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.393135\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.361750\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.330084\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.306518\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.244324\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.348609\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.290492\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.203086\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.312236\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.520450\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.434914\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.242236\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.361418\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.295000\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.320405\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.229583\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.175598\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.217386\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.365441\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.217435\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.134202\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.279018\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.196050\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.528781\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.344567\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.315258\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.383296\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.187886\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.327594\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.267832\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.282968\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.209700\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.379552\n",
            "\n",
            "Test set: Avg. loss: 0.2668, Accuracy: 9226/10000 (92%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.278229\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.257344\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.186704\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.400712\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.428577\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.273206\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.398989\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.267484\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.200285\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.187163\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.396842\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.301799\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.254693\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.208931\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.107780\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.358671\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.362621\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.243356\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.279510\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.251003\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.575602\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.173442\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.313731\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.190858\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.352965\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.225226\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.368681\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.196948\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.090549\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.219476\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.169250\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.240243\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.221804\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.165542\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.259330\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.144454\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.267824\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.385247\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.174641\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.266925\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.107148\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.378134\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.310867\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.217161\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.231133\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.145248\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.389994\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.103530\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.346443\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.169762\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.145184\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.317765\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.105380\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.297538\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.127381\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.171386\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.273672\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.247559\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.143261\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.257893\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.498470\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.203188\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.333069\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.201899\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.258604\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.274830\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.289100\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.290345\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.225657\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.187699\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.237323\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.163961\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.061529\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.350171\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.202794\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.143250\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.166211\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.174350\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.146319\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.246830\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.335176\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.218737\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.207400\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.106979\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.184605\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.381959\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.230600\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.097618\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.250307\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.192833\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.171903\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.230666\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.413885\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.158610\n",
            "\n",
            "Test set: Avg. loss: 0.2075, Accuracy: 9384/10000 (94%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-8fe0600969c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mGPU_ON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mCPU_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-5923f941ee26>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2218\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2219\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #2 'target' in call to _thnn_nll_loss_forward"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhsCZfNufYld"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYbSA3XLfbnE"
      },
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='blue')\n",
        "plt.scatter(test_counter, test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}