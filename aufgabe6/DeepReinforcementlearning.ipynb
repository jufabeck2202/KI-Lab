{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepReinforcementlearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jufabeck2202/KI-Lab/blob/main/aufgabe6/DeepReinforcementlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwm-vLKt-CxA"
      },
      "source": [
        "General idea:\n",
        "1. Use a neural network that takes as input a state (represented as numbers)\n",
        "and outputs a probability for every action.\n",
        "2. Generate episodes by inputing the current state into the network and\n",
        "sampling actions from the networkâ€™s output. Remember the\n",
        "<state, action> pairs for every episode.\n",
        "3. From these episodes, identify the ones with the highest reward.\n",
        "4. Use the <state, action> pairs from those high reward episodes as\n",
        "training examples for improving the neural network.\n",
        "5. Go back to step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28X7Ou8q-E9e"
      },
      "source": [
        "Task 2\n",
        "1. Define a neural network with two fully connected-layers. The hidden layer uses a\n",
        "Relu activation. The output layer uses a softmax. Try different hidden layer sizes\n",
        "(between 100 and 500). The network takes as input a vector of the current states\n",
        "and gives out a probability for each action.\n",
        "2. Generate 100 episodes by sampling actions using the network output. Limit the\n",
        "number of steps per episode to 500. Sum up the reward of all steps of one episode.\n",
        "3. Print out the mean reward per episode of the 100 episodes.\n",
        "4. Identify the 20 best of those episodes in terms of reward and use the\n",
        "<state, action> pairs of these episodes as training examples for the network.\n",
        "5. Update the weights of the network by performing backpropagation on these <state,\n",
        "action> pairs.\n",
        "6. Repeat steps 2 to 5 until a mean reward of 100 is reached.\n",
        "7. Record a video of the lunar lander by running the trained network on one additional\n",
        "episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQx2xvS-M-i"
      },
      "source": [
        "Hints\n",
        "* Use !pip3 install box2d-py to make the environment work in Colab.\n",
        "* You cannot show the video of your lander in Colab (env.render() fails).  \n",
        "* Workaround: Download the model on your local machine and record the video there,\n",
        "using recording_demo.py as template (see Mattermost).\n",
        "* The loss is not a useful indicator for the learning progress in RL. Instead check how\n",
        "the mean reward develops over time.\n",
        "* The mean reward will jump back and forth quite a bit, but overall should increase.\n",
        "* After roughly 70 training iterations the mean reward should be positive, and after\n",
        "roughly 100 steps be over 100.\n",
        "* Note that these numbers depend on your parameter setting and it may also take\n",
        "longer or shorter.\n",
        "* Reinforcement learning is much more difficult than supervised learning, you have to\n",
        "play around quite a bit to get things into the right direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ozuttP-6-3"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZML6vY495JLW"
      },
      "source": [
        "!pip3 install box2d-py gym > /dev/null"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLOhghCh6375"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import random\n",
        "import torch.optim as optim\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vhGJ8K17AoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab1f814-a73a-4c5a-b32d-2f45ff6f9514"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "rewards = []\n",
        "\n",
        "##get Action space\n",
        "print(env.observation_space.shape)\n",
        "print(env.action_space)\n",
        "print()\n",
        "print(env.observation_space.sample())\n",
        "print(env.action_space.sample())\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "Discrete(4)\n",
            "\n",
            "[-0.77376556  1.6299638  -1.0327146  -1.9283904   0.5361454   1.3629391\n",
            " -0.17331174 -0.706725  ]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ1t9O_D_YZh"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DA1wvKv7qLe"
      },
      "source": [
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size,):\n",
        "        super(Network, self).__init__()\n",
        "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, env.action_space.n)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_jgm5-hafJE"
      },
      "source": [
        "net = Network(200)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pcDn0VkAG4U"
      },
      "source": [
        "# Sample Episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDnOo9WzAGEx"
      },
      "source": [
        "# get episodes\n",
        "def sample(episodes_n=100, max_steps=5000):\n",
        "  episodes_data = []\n",
        "  \n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  for i_episode in range(episodes_n):\n",
        "      steps = [] # steps[0] is the action, steps[1] is the state\n",
        "      total_reward = 0\n",
        "      state = env.reset()\n",
        "      for i_step in range(max_steps):\n",
        "\n",
        "          state_vector = torch.FloatTensor([state])\n",
        "          probability= softmax(net(state_vector))\n",
        "\n",
        "          selected_action = np.random.choice(len(probability.data.numpy()[0]), p=probability.data.numpy()[0])\n",
        "          new_state, reward, done, info = env.step(selected_action)\n",
        "\n",
        "          rewards.append(reward)\n",
        "          #important: use old state\n",
        "          \n",
        "          steps.append([selected_action, state])\n",
        "          state = new_state\n",
        "          total_reward += reward\n",
        "          \n",
        "\n",
        "          if done:\n",
        "              episodes_data.append({ \"total_reward\": total_reward  ,\"steps\": steps})\n",
        "              break\n",
        "      #print(f\"Mean reward of episode {i_episode}: {np.array(rewards).mean()}\")\n",
        "  return episodes_data\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucGcZ-es9nXd"
      },
      "source": [
        "def calculate_total_mean_reward(all_episodes):\n",
        "   all_rewards = [d['total_reward'] for d in all_episodes]\n",
        "   return np.array(all_rewards).mean()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyVPXZ3xVSla"
      },
      "source": [
        "def get_top_20(all_episodes):\r\n",
        "  return heapq.nlargest(20, all_episodes, key=lambda s: s['total_reward'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw0u6RZw14nU"
      },
      "source": [
        "def filter_batch(all_episodes,percentile=80):\n",
        "\n",
        "    rewards_batch = [d['total_reward'] for d in all_episodes]\n",
        "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
        "   \n",
        "    top_20_p_episodes = []\n",
        "    for episode in all_episodes:\n",
        "      if episode['total_reward'] > reward_threshold:\n",
        "        top_20_p_episodes.append(episode)\n",
        "    return top_20_p_episodes"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S5Objpjkgaa"
      },
      "source": [
        "def get_training_data_batch(data):\n",
        "  observations = []\n",
        "  targets = []\n",
        "  for entry in data:\n",
        "    for step in entry['steps']:\n",
        "      #state\n",
        "      observations.append(step[1])\n",
        "      #action\n",
        "      targets.append(step[0])\n",
        "\n",
        "  return [observations, targets]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ZUqyjwi0kA"
      },
      "source": [
        "def train_batch():\n",
        "  \n",
        "  # get the inputs; data is a list of [inputs, labels]\n",
        "  training_data = sample(episodes_n=100)\n",
        "  best_20 = get_top_20(training_data)\n",
        "  observations, targets = get_training_data_batch(best_20)\n",
        "\n",
        "  # calculate mean for the current episodes:\n",
        "\n",
        "  #convert to torch?\n",
        "  optimizer.zero_grad()\n",
        "  observations=torch.FloatTensor(observations)\n",
        "  targets =torch.LongTensor(targets)\n",
        "  print(targets)\n",
        "\n",
        "  # forward + backward + optimize\n",
        "  action_pred = net(observations)\n",
        "\n",
        "  loss = criterion(action_pred, targets)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  reward_complete = calculate_total_mean_reward(training_data)\n",
        "  return reward_complete, loss\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk4n721CILBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a5cdafa-981b-4386-b6e3-ab7eed62c177"
      },
      "source": [
        "i = 0\n",
        "rewards = []\n",
        "losses = []\n",
        "while True:\n",
        "  i+=1\n",
        "  reward, loss = train_batch()\n",
        "  print(f\"{i}: Mean Reward for Sampling 100 episodes {reward}, Loss: {loss}\")\n",
        "  rewards.append(reward)\n",
        "  losses.append(loss)\n",
        "\n",
        "  if 150 < reward:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3, 2, 1,  ..., 1, 3, 0])\n",
            "1: Mean Reward for Sampling 100 episodes -134.36186507142784, Loss: 1.3303700685501099\n",
            "tensor([0, 0, 0,  ..., 3, 1, 0])\n",
            "2: Mean Reward for Sampling 100 episodes -170.948173889581, Loss: 1.339107632637024\n",
            "tensor([3, 3, 2,  ..., 1, 2, 2])\n",
            "3: Mean Reward for Sampling 100 episodes -155.79894817322835, Loss: 1.3438349962234497\n",
            "tensor([3, 3, 2,  ..., 0, 0, 0])\n",
            "4: Mean Reward for Sampling 100 episodes -152.09954459593183, Loss: 1.3341596126556396\n",
            "tensor([0, 0, 3,  ..., 1, 3, 1])\n",
            "5: Mean Reward for Sampling 100 episodes -135.86150995626912, Loss: 1.326799750328064\n",
            "tensor([0, 1, 2,  ..., 2, 1, 1])\n",
            "6: Mean Reward for Sampling 100 episodes -109.71902938339747, Loss: 1.2987562417984009\n",
            "tensor([3, 1, 0,  ..., 0, 2, 2])\n",
            "7: Mean Reward for Sampling 100 episodes -141.2439338084876, Loss: 1.2695581912994385\n",
            "tensor([0, 2, 2,  ..., 2, 2, 2])\n",
            "8: Mean Reward for Sampling 100 episodes -127.611744142535, Loss: 1.240256667137146\n",
            "tensor([0, 1, 0,  ..., 2, 1, 1])\n",
            "9: Mean Reward for Sampling 100 episodes -127.31709222758217, Loss: 1.2571310997009277\n",
            "tensor([3, 1, 0,  ..., 2, 0, 0])\n",
            "10: Mean Reward for Sampling 100 episodes -102.5134382034916, Loss: 1.2033401727676392\n",
            "tensor([2, 0, 2,  ..., 2, 2, 2])\n",
            "11: Mean Reward for Sampling 100 episodes -104.02058804616222, Loss: 1.2229526042938232\n",
            "tensor([0, 1, 1,  ..., 1, 2, 2])\n",
            "12: Mean Reward for Sampling 100 episodes -74.1320177554556, Loss: 1.2269905805587769\n",
            "tensor([0, 1, 1,  ..., 2, 2, 0])\n",
            "13: Mean Reward for Sampling 100 episodes -82.88205322015396, Loss: 1.214084506034851\n",
            "tensor([3, 0, 1,  ..., 2, 1, 0])\n",
            "14: Mean Reward for Sampling 100 episodes -65.4201835952958, Loss: 1.1921656131744385\n",
            "tensor([2, 0, 2,  ..., 2, 1, 2])\n",
            "15: Mean Reward for Sampling 100 episodes -66.45242968295148, Loss: 1.1705576181411743\n",
            "tensor([3, 0, 0,  ..., 0, 1, 2])\n",
            "16: Mean Reward for Sampling 100 episodes -86.82226680080593, Loss: 1.114230990409851\n",
            "tensor([0, 3, 0,  ..., 2, 2, 2])\n",
            "17: Mean Reward for Sampling 100 episodes -81.62946178249602, Loss: 1.081305980682373\n",
            "tensor([0, 2, 0,  ..., 2, 2, 2])\n",
            "18: Mean Reward for Sampling 100 episodes -61.362483253419526, Loss: 1.1364411115646362\n",
            "tensor([2, 0, 0,  ..., 0, 2, 2])\n",
            "19: Mean Reward for Sampling 100 episodes -45.48921176133071, Loss: 1.1756949424743652\n",
            "tensor([1, 0, 1,  ..., 2, 2, 2])\n",
            "20: Mean Reward for Sampling 100 episodes -65.52740335659895, Loss: 1.163405179977417\n",
            "tensor([0, 2, 1,  ..., 3, 2, 3])\n",
            "21: Mean Reward for Sampling 100 episodes -97.56973123838875, Loss: 1.0486090183258057\n",
            "tensor([3, 0, 1,  ..., 2, 2, 2])\n",
            "22: Mean Reward for Sampling 100 episodes -58.036908377389764, Loss: 1.186774492263794\n",
            "tensor([0, 0, 3,  ..., 3, 0, 3])\n",
            "23: Mean Reward for Sampling 100 episodes -81.78118999291159, Loss: 1.1157751083374023\n",
            "tensor([3, 0, 0,  ..., 0, 2, 2])\n",
            "24: Mean Reward for Sampling 100 episodes -79.24004759473252, Loss: 1.146518349647522\n",
            "tensor([0, 0, 3,  ..., 3, 0, 3])\n",
            "25: Mean Reward for Sampling 100 episodes -65.37077105188311, Loss: 1.1512579917907715\n",
            "tensor([3, 0, 0,  ..., 2, 3, 2])\n",
            "26: Mean Reward for Sampling 100 episodes -49.77010865309623, Loss: 1.1728938817977905\n",
            "tensor([3, 3, 0,  ..., 0, 2, 3])\n",
            "27: Mean Reward for Sampling 100 episodes -52.4643000249414, Loss: 1.1979150772094727\n",
            "tensor([3, 0, 3,  ..., 0, 3, 0])\n",
            "28: Mean Reward for Sampling 100 episodes -57.685351923600734, Loss: 1.1916143894195557\n",
            "tensor([3, 0, 3,  ..., 2, 1, 2])\n",
            "29: Mean Reward for Sampling 100 episodes -26.067550693874452, Loss: 1.1949268579483032\n",
            "tensor([3, 0, 0,  ..., 2, 0, 1])\n",
            "30: Mean Reward for Sampling 100 episodes -24.396729784624405, Loss: 1.2240744829177856\n",
            "tensor([3, 1, 1,  ..., 2, 1, 2])\n",
            "31: Mean Reward for Sampling 100 episodes -31.572045101117055, Loss: 1.197237491607666\n",
            "tensor([3, 3, 0,  ..., 2, 0, 2])\n",
            "32: Mean Reward for Sampling 100 episodes -15.320301668877043, Loss: 1.1467705965042114\n",
            "tensor([1, 1, 1,  ..., 3, 2, 2])\n",
            "33: Mean Reward for Sampling 100 episodes -24.206088436472946, Loss: 1.1265367269515991\n",
            "tensor([0, 1, 1,  ..., 2, 2, 2])\n",
            "34: Mean Reward for Sampling 100 episodes -15.263155679120887, Loss: 1.109601378440857\n",
            "tensor([1, 1, 3,  ..., 3, 3, 3])\n",
            "35: Mean Reward for Sampling 100 episodes -6.232168340150746, Loss: 1.1378536224365234\n",
            "tensor([0, 3, 0,  ..., 3, 3, 3])\n",
            "36: Mean Reward for Sampling 100 episodes -1.6711513986751934, Loss: 1.1258357763290405\n",
            "tensor([1, 1, 1,  ..., 3, 2, 3])\n",
            "37: Mean Reward for Sampling 100 episodes 5.845686190563069, Loss: 1.1009092330932617\n",
            "tensor([3, 3, 3,  ..., 2, 3, 0])\n",
            "38: Mean Reward for Sampling 100 episodes -1.9710897077332026, Loss: 1.042164921760559\n",
            "tensor([0, 2, 0,  ..., 3, 2, 3])\n",
            "39: Mean Reward for Sampling 100 episodes 13.801595888914889, Loss: 1.047477126121521\n",
            "tensor([1, 1, 1,  ..., 2, 0, 3])\n",
            "40: Mean Reward for Sampling 100 episodes 19.380070272435674, Loss: 0.9679694175720215\n",
            "tensor([2, 3, 1,  ..., 3, 3, 3])\n",
            "41: Mean Reward for Sampling 100 episodes 25.316942508195936, Loss: 0.9445564150810242\n",
            "tensor([3, 3, 3,  ..., 2, 1, 2])\n",
            "42: Mean Reward for Sampling 100 episodes 22.700000288210013, Loss: 0.8821682929992676\n",
            "tensor([1, 0, 0,  ..., 3, 2, 3])\n",
            "43: Mean Reward for Sampling 100 episodes 16.439483490070575, Loss: 0.9033337831497192\n",
            "tensor([1, 1, 2,  ..., 3, 1, 2])\n",
            "44: Mean Reward for Sampling 100 episodes 21.715273082568636, Loss: 0.9056263566017151\n",
            "tensor([0, 3, 3,  ..., 3, 2, 0])\n",
            "45: Mean Reward for Sampling 100 episodes 16.57733697610317, Loss: 0.9376157522201538\n",
            "tensor([0, 0, 3,  ..., 1, 1, 1])\n",
            "46: Mean Reward for Sampling 100 episodes 26.59426397534751, Loss: 0.9484410881996155\n",
            "tensor([1, 1, 1,  ..., 3, 3, 3])\n",
            "47: Mean Reward for Sampling 100 episodes 8.86582504506394, Loss: 0.9466294646263123\n",
            "tensor([2, 1, 1,  ..., 3, 3, 3])\n",
            "48: Mean Reward for Sampling 100 episodes 36.98606277084232, Loss: 0.8367566466331482\n",
            "tensor([1, 0, 1,  ..., 3, 0, 1])\n",
            "49: Mean Reward for Sampling 100 episodes 32.87591632736819, Loss: 0.7809316515922546\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "50: Mean Reward for Sampling 100 episodes 23.178179948254876, Loss: 0.7417881488800049\n",
            "tensor([3, 0, 3,  ..., 3, 3, 3])\n",
            "51: Mean Reward for Sampling 100 episodes 34.735184818522335, Loss: 0.7546072006225586\n",
            "tensor([3, 0, 1,  ..., 3, 3, 3])\n",
            "52: Mean Reward for Sampling 100 episodes 35.683898045755484, Loss: 0.7174871563911438\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "53: Mean Reward for Sampling 100 episodes 20.311813457485336, Loss: 0.744853675365448\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "54: Mean Reward for Sampling 100 episodes 15.766870649624714, Loss: 0.7399347424507141\n",
            "tensor([3, 3, 0,  ..., 3, 3, 3])\n",
            "55: Mean Reward for Sampling 100 episodes 54.205422598147464, Loss: 0.7631595730781555\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "56: Mean Reward for Sampling 100 episodes 45.983134995950614, Loss: 0.7711142897605896\n",
            "tensor([3, 3, 1,  ..., 3, 3, 3])\n",
            "57: Mean Reward for Sampling 100 episodes 50.892732115426185, Loss: 0.7366337180137634\n",
            "tensor([0, 2, 1,  ..., 3, 3, 3])\n",
            "58: Mean Reward for Sampling 100 episodes 74.70973143239695, Loss: 0.7548741102218628\n",
            "tensor([3, 3, 1,  ..., 3, 3, 3])\n",
            "59: Mean Reward for Sampling 100 episodes 94.72379160669479, Loss: 0.727932333946228\n",
            "tensor([0, 3, 3,  ..., 3, 3, 3])\n",
            "60: Mean Reward for Sampling 100 episodes 79.72865422940491, Loss: 0.7649586796760559\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "61: Mean Reward for Sampling 100 episodes 65.96673400300286, Loss: 0.7477140426635742\n",
            "tensor([0, 0, 1,  ..., 3, 3, 3])\n",
            "62: Mean Reward for Sampling 100 episodes 78.40511181832767, Loss: 0.7735108137130737\n",
            "tensor([1, 2, 1,  ..., 3, 3, 3])\n",
            "63: Mean Reward for Sampling 100 episodes 78.3564990505787, Loss: 0.7448689937591553\n",
            "tensor([3, 0, 3,  ..., 3, 3, 3])\n",
            "64: Mean Reward for Sampling 100 episodes 70.63102314446921, Loss: 0.6976216435432434\n",
            "tensor([1, 1, 1,  ..., 3, 3, 3])\n",
            "65: Mean Reward for Sampling 100 episodes 81.54343147788308, Loss: 0.6887571215629578\n",
            "tensor([3, 1, 3,  ..., 3, 3, 3])\n",
            "66: Mean Reward for Sampling 100 episodes 59.01593299137318, Loss: 0.674988329410553\n",
            "tensor([3, 0, 3,  ..., 3, 3, 3])\n",
            "67: Mean Reward for Sampling 100 episodes 62.82388832543406, Loss: 0.6409107446670532\n",
            "tensor([1, 1, 1,  ..., 3, 3, 3])\n",
            "68: Mean Reward for Sampling 100 episodes 66.03205003325903, Loss: 0.6494964957237244\n",
            "tensor([3, 0, 3,  ..., 3, 3, 3])\n",
            "69: Mean Reward for Sampling 100 episodes 54.150523010872575, Loss: 0.6289331912994385\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "70: Mean Reward for Sampling 100 episodes 74.12825061944267, Loss: 0.6144071221351624\n",
            "tensor([1, 1, 1,  ..., 3, 3, 3])\n",
            "71: Mean Reward for Sampling 100 episodes 46.94915918534087, Loss: 0.5867049098014832\n",
            "tensor([3, 0, 0,  ..., 3, 3, 3])\n",
            "72: Mean Reward for Sampling 100 episodes 78.8443136992137, Loss: 0.641685962677002\n",
            "tensor([3, 3, 0,  ..., 3, 3, 3])\n",
            "73: Mean Reward for Sampling 100 episodes 74.9363570671226, Loss: 0.5727127194404602\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "74: Mean Reward for Sampling 100 episodes 76.27909858264219, Loss: 0.6124120354652405\n",
            "tensor([3, 0, 3,  ..., 3, 3, 3])\n",
            "75: Mean Reward for Sampling 100 episodes 76.96097668273562, Loss: 0.6058663725852966\n",
            "tensor([1, 1, 1,  ..., 3, 3, 3])\n",
            "76: Mean Reward for Sampling 100 episodes 84.63905223000795, Loss: 0.6020804047584534\n",
            "tensor([3, 0, 0,  ..., 3, 3, 3])\n",
            "77: Mean Reward for Sampling 100 episodes 110.7129034507275, Loss: 0.6420422196388245\n",
            "tensor([1, 3, 1,  ..., 3, 3, 3])\n",
            "78: Mean Reward for Sampling 100 episodes 80.53377010864538, Loss: 0.587909460067749\n",
            "tensor([1, 1, 1,  ..., 3, 3, 3])\n",
            "79: Mean Reward for Sampling 100 episodes 126.36684367780308, Loss: 0.632226824760437\n",
            "tensor([1, 1, 1,  ..., 3, 3, 3])\n",
            "80: Mean Reward for Sampling 100 episodes 111.07200567270515, Loss: 0.5898873805999756\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "81: Mean Reward for Sampling 100 episodes 106.51868136496465, Loss: 0.6805177927017212\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "82: Mean Reward for Sampling 100 episodes 133.04715854714496, Loss: 0.6112240552902222\n",
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "83: Mean Reward for Sampling 100 episodes 100.75141414141716, Loss: 0.6355011463165283\n",
            "tensor([3, 3, 3,  ..., 3, 3, 2])\n",
            "84: Mean Reward for Sampling 100 episodes 96.15450649607723, Loss: 0.652087926864624\n",
            "tensor([1, 3, 3,  ..., 3, 1, 1])\n",
            "85: Mean Reward for Sampling 100 episodes 77.86486008025247, Loss: 0.7547033429145813\n",
            "tensor([1, 1, 1,  ..., 1, 1, 3])\n",
            "86: Mean Reward for Sampling 100 episodes 81.22150999757642, Loss: 0.7542704939842224\n",
            "tensor([3, 3, 0,  ..., 3, 3, 1])\n",
            "87: Mean Reward for Sampling 100 episodes 68.44165206602847, Loss: 0.7821784019470215\n",
            "tensor([1, 0, 3,  ..., 3, 1, 1])\n",
            "88: Mean Reward for Sampling 100 episodes 83.4830190523794, Loss: 0.764953076839447\n",
            "tensor([3, 3, 3,  ..., 3, 1, 1])\n",
            "89: Mean Reward for Sampling 100 episodes 82.18498545640625, Loss: 0.7523356080055237\n",
            "tensor([0, 3, 3,  ..., 1, 1, 3])\n",
            "90: Mean Reward for Sampling 100 episodes 79.40653572674397, Loss: 0.6887921094894409\n",
            "tensor([3, 0, 3,  ..., 1, 1, 3])\n",
            "91: Mean Reward for Sampling 100 episodes 81.28409401303492, Loss: 0.6750000715255737\n",
            "tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "92: Mean Reward for Sampling 100 episodes 99.86391202311891, Loss: 0.5985942482948303\n",
            "tensor([3, 3, 3,  ..., 1, 1, 1])\n",
            "93: Mean Reward for Sampling 100 episodes 90.65636469885223, Loss: 0.6046322584152222\n",
            "tensor([3, 3, 3,  ..., 1, 1, 1])\n",
            "94: Mean Reward for Sampling 100 episodes 100.36154568244747, Loss: 0.5609238147735596\n",
            "tensor([0, 3, 3,  ..., 1, 1, 1])\n",
            "95: Mean Reward for Sampling 100 episodes 110.37223055914248, Loss: 0.5754473805427551\n",
            "tensor([3, 0, 3,  ..., 1, 1, 1])\n",
            "96: Mean Reward for Sampling 100 episodes 95.10257310646031, Loss: 0.5916338562965393\n",
            "tensor([3, 3, 3,  ..., 1, 1, 1])\n",
            "97: Mean Reward for Sampling 100 episodes 103.65316767040589, Loss: 0.5500915050506592\n",
            "tensor([3, 3, 0,  ..., 1, 1, 1])\n",
            "98: Mean Reward for Sampling 100 episodes 109.6014428188708, Loss: 0.5730702877044678\n",
            "tensor([3, 3, 3,  ..., 1, 1, 1])\n",
            "99: Mean Reward for Sampling 100 episodes 108.19893685788152, Loss: 0.534073531627655\n",
            "tensor([3, 3, 3,  ..., 1, 1, 1])\n",
            "100: Mean Reward for Sampling 100 episodes 102.1242067940423, Loss: 0.5540621280670166\n",
            "tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "101: Mean Reward for Sampling 100 episodes 118.12188811285097, Loss: 0.5675641298294067\n",
            "tensor([3, 3, 3,  ..., 1, 1, 1])\n",
            "102: Mean Reward for Sampling 100 episodes 101.00129761804496, Loss: 0.6050411462783813\n",
            "tensor([3, 3, 3,  ..., 1, 1, 1])\n",
            "103: Mean Reward for Sampling 100 episodes 119.34386148776503, Loss: 0.6170057654380798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwS-Yt3UsRUq"
      },
      "source": [
        "plt.plot(rewards)\n",
        "plt.ylabel('Rewards')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmi_7YKatg6m"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.ylabel(losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbdTOUSLbQ_r"
      },
      "source": [
        "# Deprecated code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc6nClmuXcgX"
      },
      "source": [
        "# depricated code\n",
        "def get_training_data(episode_data):\n",
        "  train_data = []\n",
        "  for entry in episodes_data:\n",
        "    for step in entry['steps']:\n",
        "      one_hot = np.zeros(env.action_space.n)\n",
        "      one_hot[step[0]] = 1\n",
        "      train_data.append([one_hot, step[1]])\n",
        "  return train_data\n",
        "\n",
        "def train(training_data):\n",
        "  for data in training_data:\n",
        "    optimizer.zero_grad()\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    action, observation = data\n",
        "    #convert to torch?\n",
        "    observation=torch.from_numpy(np.expand_dims(observation, axis=0))\n",
        "    action =torch.from_numpy(np.array([np.argmax(action, axis=0)]))\n",
        "\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    output_action = net(observation)\n",
        "    loss = criterion(output_action, action)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "while False:\n",
        "  episodes_data = sample(episodes_n=200)\n",
        "  print(f\"Mean Reward for Sampling 20 episodes {calculate_total_mean_reward(episodes_data)}\")\n",
        "  best_20 = get_top_20(episodes_data)\n",
        "  train(get_training_data(best_20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMu2VyYHZItY"
      },
      "source": [
        "# Download Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOYH3BEWdSRq"
      },
      "source": [
        "Code found [here](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb#scrollTo=78BfQoQKOq7z)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF92FCzZMWPn"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2499HTRYX2n"
      },
      "source": [
        "## used for downloading videos\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqf4kCnjYktY"
      },
      "source": [
        "env = wrap_env(gym.make(\"LunarLander-v2\"))\n",
        "softmax = nn.Softmax(dim=1)\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    state_vector = torch.FloatTensor([observation])\n",
        "    probability= softmax(net(state_vector))\n",
        "\n",
        "    selected_action = np.random.choice(len(probability.data.numpy()[0]), p=probability.data.numpy()[0])\n",
        "    new_state, reward, done, info = env.step(selected_action)\n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}