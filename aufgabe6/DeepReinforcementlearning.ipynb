{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepReinforcementlearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jufabeck2202/KI-Lab/blob/main/aufgabe6/DeepReinforcementlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwm-vLKt-CxA"
      },
      "source": [
        "General idea:\n",
        "1. Use a neural network that takes as input a state (represented as numbers)\n",
        "and outputs a probability for every action.\n",
        "2. Generate episodes by inputing the current state into the network and\n",
        "sampling actions from the networkâ€™s output. Remember the\n",
        "<state, action> pairs for every episode.\n",
        "3. From these episodes, identify the ones with the highest reward.\n",
        "4. Use the <state, action> pairs from those high reward episodes as\n",
        "training examples for improving the neural network.\n",
        "5. Go back to step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28X7Ou8q-E9e"
      },
      "source": [
        "Task 2\n",
        "1. Define a neural network with two fully connected-layers. The hidden layer uses a\n",
        "Relu activation. The output layer uses a softmax. Try different hidden layer sizes\n",
        "(between 100 and 500). The network takes as input a vector of the current states\n",
        "and gives out a probability for each action.\n",
        "2. Generate 100 episodes by sampling actions using the network output. Limit the\n",
        "number of steps per episode to 500. Sum up the reward of all steps of one episode.\n",
        "3. Print out the mean reward per episode of the 100 episodes.\n",
        "4. Identify the 20 best of those episodes in terms of reward and use the\n",
        "<state, action> pairs of these episodes as training examples for the network.\n",
        "5. Update the weights of the network by performing backpropagation on these <state,\n",
        "action> pairs.\n",
        "6. Repeat steps 2 to 5 until a mean reward of 100 is reached.\n",
        "7. Record a video of the lunar lander by running the trained network on one additional\n",
        "episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQx2xvS-M-i"
      },
      "source": [
        "Hints\n",
        "* Use !pip3 install box2d-py to make the environment work in Colab.\n",
        "* You cannot show the video of your lander in Colab (env.render() fails).  \n",
        "* Workaround: Download the model on your local machine and record the video there,\n",
        "using recording_demo.py as template (see Mattermost).\n",
        "* The loss is not a useful indicator for the learning progress in RL. Instead check how\n",
        "the mean reward develops over time.\n",
        "* The mean reward will jump back and forth quite a bit, but overall should increase.\n",
        "* After roughly 70 training iterations the mean reward should be positive, and after\n",
        "roughly 100 steps be over 100.\n",
        "* Note that these numbers depend on your parameter setting and it may also take\n",
        "longer or shorter.\n",
        "* Reinforcement learning is much more difficult than supervised learning, you have to\n",
        "play around quite a bit to get things into the right direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ozuttP-6-3"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZML6vY495JLW"
      },
      "source": [
        "!pip3 install box2d-py gym > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLOhghCh6375"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vhGJ8K17AoF",
        "outputId": "acbd78ab-bb66-4a19-8afe-9a26a8e9a520"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "rewards = []\n",
        "\n",
        "##get Action space\n",
        "print(env.observation_space.shape)\n",
        "print(env.action_space)\n",
        "print()\n",
        "print(env.observation_space.sample())\n",
        "print(env.action_space.sample())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "Discrete(4)\n",
            "\n",
            "[-0.15572956 -0.353235   -0.1183357  -0.13081883  0.6019697  -0.8882228\n",
            "  0.26233912 -2.2259324 ]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ1t9O_D_YZh"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_jgm5-hafJE",
        "outputId": "977c7643-7da9-45cd-d0ca-ed8481228875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.backends.cudnn.enabled = True\r\n",
        "GPU_ON = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda:0\" if GPU_ON else \"cpu\")\r\n",
        "device"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DA1wvKv7qLe"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, hidden):\n",
        "        super(Network, self).__init__()\n",
        "        # input == observation space, for this problem its 8\n",
        "        self.linear1 = torch.nn.Linear(env.observation_space.shape[0], hidden)\n",
        "        # action == action space, up down left right for this problem, so 4\n",
        "        self.linear2 = torch.nn.Linear(hidden, env.action_space.n)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hidden = f.relu(self.linear1(state))\n",
        "        return f.softmax(self.linear2(hidden)) "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuVDhm_DahrL"
      },
      "source": [
        "def train(training_data):\r\n",
        "    for action, observation in enumerate(training_data, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        if GPU_ON:\r\n",
        "          action = action.cuda()\r\n",
        "          observation = observation.cuda()\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(observation)\r\n",
        "        loss = criterion(outputs, action)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pcDn0VkAG4U"
      },
      "source": [
        "# Sample Episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDnOo9WzAGEx"
      },
      "source": [
        "def sample(episodes_n=100, max_steps=500):\n",
        "  episodes_data = []\n",
        "  for i_episode in range(episodes_n):\n",
        "      steps = []\n",
        "      rewards = []\n",
        "      observation = env.reset()\n",
        "      for t in range(max_steps):\n",
        "          action = env.action_space.sample()\n",
        "          observation, reward, done, info = env.step(action)\n",
        "          rewards.append(reward)\n",
        "          steps.append([action, observation])\n",
        "          if done:\n",
        "              break\n",
        "      print(f\"Mean reward of episode {i_episode}: {np.array(rewards).mean()}\")\n",
        "      episodes_data.append({ \"total_reward\": np.array(rewards).sum() ,\"steps\": steps})\n",
        "  return np.array(episodes_data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sERYGp7WirM",
        "outputId": "a7fcc2e5-15b8-4104-c0c9-95c5643cada2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "episodes_data = sample()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward of episode 0: -1.8671913423563917\n",
            "Mean reward of episode 1: -1.2933785652526817\n",
            "Mean reward of episode 2: -1.3527540171949266\n",
            "Mean reward of episode 3: -3.079354920344859\n",
            "Mean reward of episode 4: -2.767937045128633\n",
            "Mean reward of episode 5: -2.125392769794649\n",
            "Mean reward of episode 6: -1.689989581339652\n",
            "Mean reward of episode 7: -0.3373901094490223\n",
            "Mean reward of episode 8: -1.0452666530154997\n",
            "Mean reward of episode 9: -2.3270546329412616\n",
            "Mean reward of episode 10: -0.871604878093666\n",
            "Mean reward of episode 11: -1.9051051518316524\n",
            "Mean reward of episode 12: -0.8645924647355984\n",
            "Mean reward of episode 13: -3.6880257965894003\n",
            "Mean reward of episode 14: -3.250509963309707\n",
            "Mean reward of episode 15: -2.6520184786944614\n",
            "Mean reward of episode 16: -4.948416313515281\n",
            "Mean reward of episode 17: -1.1253923274732167\n",
            "Mean reward of episode 18: -0.8902477653690054\n",
            "Mean reward of episode 19: -1.478248514381291\n",
            "Mean reward of episode 20: -2.5758432158338276\n",
            "Mean reward of episode 21: -0.9149509274025974\n",
            "Mean reward of episode 22: -1.426031657527807\n",
            "Mean reward of episode 23: -1.27162348225673\n",
            "Mean reward of episode 24: -1.6122577650299752\n",
            "Mean reward of episode 25: -4.037392919816482\n",
            "Mean reward of episode 26: -3.9463829965093047\n",
            "Mean reward of episode 27: -1.505454208291596\n",
            "Mean reward of episode 28: -1.166611542348456\n",
            "Mean reward of episode 29: -3.4611020968581467\n",
            "Mean reward of episode 30: -2.7035516346323383\n",
            "Mean reward of episode 31: -2.811533768938998\n",
            "Mean reward of episode 32: -1.1211371663547451\n",
            "Mean reward of episode 33: 0.02456363560170322\n",
            "Mean reward of episode 34: -3.178200442988189\n",
            "Mean reward of episode 35: -1.245796116335661\n",
            "Mean reward of episode 36: -1.7912209634826348\n",
            "Mean reward of episode 37: -1.676199391010134\n",
            "Mean reward of episode 38: -1.4921064059129068\n",
            "Mean reward of episode 39: -1.686497542004821\n",
            "Mean reward of episode 40: -1.7376133553197313\n",
            "Mean reward of episode 41: -2.778993280580685\n",
            "Mean reward of episode 42: -3.279897166826673\n",
            "Mean reward of episode 43: -2.6046357619359086\n",
            "Mean reward of episode 44: -3.499723878638572\n",
            "Mean reward of episode 45: -3.0909456218218376\n",
            "Mean reward of episode 46: -2.2431506197565216\n",
            "Mean reward of episode 47: -2.7199073485969705\n",
            "Mean reward of episode 48: -1.0965358226149542\n",
            "Mean reward of episode 49: -2.870593850220381\n",
            "Mean reward of episode 50: -1.2793622449945612\n",
            "Mean reward of episode 51: -1.0168570820619376\n",
            "Mean reward of episode 52: -4.2095945671767225\n",
            "Mean reward of episode 53: -3.8101433728725187\n",
            "Mean reward of episode 54: -0.5394973525449731\n",
            "Mean reward of episode 55: -0.9310765629035263\n",
            "Mean reward of episode 56: -2.360363940016337\n",
            "Mean reward of episode 57: -2.0666739704996124\n",
            "Mean reward of episode 58: -0.18266462544188755\n",
            "Mean reward of episode 59: -1.8874335019778032\n",
            "Mean reward of episode 60: -2.2825390741994327\n",
            "Mean reward of episode 61: -3.5111467557494613\n",
            "Mean reward of episode 62: -3.4271100380918074\n",
            "Mean reward of episode 63: -2.7723026261849557\n",
            "Mean reward of episode 64: -1.984051888430439\n",
            "Mean reward of episode 65: -3.3242304167662966\n",
            "Mean reward of episode 66: -1.1328002916605295\n",
            "Mean reward of episode 67: -3.253034475534375\n",
            "Mean reward of episode 68: -0.7037097068963262\n",
            "Mean reward of episode 69: -1.0775057089890052\n",
            "Mean reward of episode 70: -4.97867636624863\n",
            "Mean reward of episode 71: -1.0703807940930392\n",
            "Mean reward of episode 72: -2.515800311103487\n",
            "Mean reward of episode 73: -3.6396476071192714\n",
            "Mean reward of episode 74: -1.0588636580484392\n",
            "Mean reward of episode 75: -0.995396283816898\n",
            "Mean reward of episode 76: -2.3471176355961125\n",
            "Mean reward of episode 77: -3.2051118854511613\n",
            "Mean reward of episode 78: -1.8100020412960984\n",
            "Mean reward of episode 79: -1.207386811365228\n",
            "Mean reward of episode 80: -1.2938204200637111\n",
            "Mean reward of episode 81: -2.5190016788277343\n",
            "Mean reward of episode 82: -1.6595443435862163\n",
            "Mean reward of episode 83: -1.6934926877606509\n",
            "Mean reward of episode 84: -3.564913963328134\n",
            "Mean reward of episode 85: -1.164596008159648\n",
            "Mean reward of episode 86: -3.1031716760737806\n",
            "Mean reward of episode 87: -2.529813557605765\n",
            "Mean reward of episode 88: -1.252061899780364\n",
            "Mean reward of episode 89: -0.9495330580036829\n",
            "Mean reward of episode 90: -3.906642271563625\n",
            "Mean reward of episode 91: -3.9065280099694806\n",
            "Mean reward of episode 92: -1.435945593054579\n",
            "Mean reward of episode 93: -2.251552715745474\n",
            "Mean reward of episode 94: -0.13131653084937014\n",
            "Mean reward of episode 95: -2.582282481751201\n",
            "Mean reward of episode 96: -1.1365638206136377\n",
            "Mean reward of episode 97: -1.3689396712767916\n",
            "Mean reward of episode 98: -1.1363812518406329\n",
            "Mean reward of episode 99: -2.159109055541733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucGcZ-es9nXd"
      },
      "source": [
        "def calculate_total_mean_reward(episode_data):\n",
        "   all_rewards = [d['total_reward'] for d in episode_data]\n",
        "   return np.array(all_rewards).mean()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGuoGeK4Dff7",
        "outputId": "862418ee-21a1-4cbf-e792-a4fd9815c598"
      },
      "source": [
        "calculate_total_mean_reward(episodes_data)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-203.85825702792033"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyVPXZ3xVSla",
        "outputId": "4c8551c1-4b57-4e2a-9265-70bf42696039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_20 = heapq.nlargest(20, episodes_data, key=lambda s: s['total_reward'])\r\n",
        "calculate_total_mean_reward(best_20)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-68.25264381334406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0swcXlC9ZGLS",
        "outputId": "2fa2ad87-4fb6-4fb6-e794-04f2598f68b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_20[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'steps': [[0,\n",
              "   array([-0.00818567,  1.4014179 , -0.4139943 , -0.22400075,  0.0093892 ,\n",
              "           0.09281062,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.01227884,  1.3957785 , -0.4140088 , -0.2506794 ,  0.01402657,\n",
              "           0.09275611,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.0164587 ,  1.3895322 , -0.42488346, -0.2776981 ,  0.02084308,\n",
              "           0.13634305,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.02073212,  1.3837118 , -0.43384618, -0.2587809 ,  0.02727913,\n",
              "           0.12873283,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.02491045,  1.3772984 , -0.42191187, -0.2851285 ,  0.03131577,\n",
              "           0.08074023,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.02908888,  1.3702849 , -0.4219226 , -0.31179398,  0.03535268,\n",
              "           0.08074541,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.0332675 ,  1.3626717 , -0.4219347 , -0.33846295,  0.03938843,\n",
              "           0.08072238,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.03739033,  1.3559633 , -0.41680914, -0.2982787 ,  0.04387686,\n",
              "           0.08977672,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.04159298,  1.348649  , -0.4268314 , -0.3252837 ,  0.0503743 ,\n",
              "           0.12996076,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.04582453,  1.3418053 , -0.4297499 , -0.3044091 ,  0.05690429,\n",
              "           0.13061173,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.04994135,  1.3352408 , -0.41897646, -0.29204816,  0.06413464,\n",
              "           0.14462014,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.05423136,  1.328978  , -0.4356585 , -0.2786479 ,  0.07072895,\n",
              "           0.13189828,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.05852547,  1.3233749 , -0.43632656, -0.24936405,  0.07758439,\n",
              "           0.13712157,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.06281976,  1.3171723 , -0.4363455 , -0.27604333,  0.08443832,\n",
              "           0.1370912 ,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.06702948,  1.3103759 , -0.4257161 , -0.30233926,  0.08915283,\n",
              "           0.09429874,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.0711544 ,  1.3029965 , -0.41508168, -0.32812718,  0.09171262,\n",
              "           0.05120088,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.07519169,  1.2960334 , -0.40700632, -0.3096747 ,  0.09497327,\n",
              "           0.06521843,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.07914839,  1.2895186 , -0.39963418, -0.28981006,  0.09891818,\n",
              "           0.07890536,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.08316956,  1.2823877 , -0.4077366 , -0.3173038 ,  0.10450628,\n",
              "           0.11177178,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.087113  ,  1.2746775 , -0.39796048, -0.34293798,  0.10810111,\n",
              "           0.07190326,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.09105654,  1.2663674 , -0.3979693 , -0.36960843,  0.11169659,\n",
              "           0.07191578,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.09491453,  1.2574806 , -0.38720495, -0.39506626,  0.11309497,\n",
              "           0.02797004,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.09867859,  1.248007  , -0.37542817, -0.4209784 ,  0.11211712,\n",
              "          -0.01955863,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.10253553,  1.2379107 , -0.3870945 , -0.44883245,  0.11351297,\n",
              "           0.02791926,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.10629702,  1.2272038 , -0.37517998, -0.47578588,  0.1125406 ,\n",
              "          -0.01944721,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.11019354,  1.2170131 , -0.38842997, -0.45282942,  0.11133442,\n",
              "          -0.02412328,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.11407232,  1.2077699 , -0.38728473, -0.41076663,  0.11073871,\n",
              "          -0.01191459,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.11803188,  1.1979026 , -0.39744595, -0.43865865,  0.11221671,\n",
              "           0.02956023,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.12206888,  1.1874324 , -0.40714654, -0.46560088,  0.11563674,\n",
              "           0.06840091,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.12618169,  1.1763408 , -0.41667324, -0.49338955,  0.12100092,\n",
              "           0.10728337,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.13037519,  1.1646243 , -0.42683762, -0.5213618 ,  0.1284454 ,\n",
              "           0.14888963,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.13464232,  1.15229   , -0.43607306, -0.5490226 ,  0.13777228,\n",
              "           0.18653724,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.13882141,  1.1393659 , -0.42503452, -0.5750852 ,  0.14487804,\n",
              "           0.14211573,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.14307432,  1.1258261 , -0.43430528, -0.6026742 ,  0.15387124,\n",
              "           0.17986389,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.14740753,  1.1116654 , -0.44438228, -0.63054883,  0.16492854,\n",
              "           0.2211459 ,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.1516819 ,  1.0969213 , -0.436953  , -0.6563761 ,  0.17446586,\n",
              "           0.1907469 ,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.15595636,  1.0815785 , -0.43695107, -0.6830502 ,  0.18400316,\n",
              "           0.1907457 ,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.16030264,  1.0656238 , -0.44594198, -0.71054727,  0.19536774,\n",
              "           0.22729187,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.16458349,  1.0490893 , -0.43767697, -0.73616064,  0.2050311 ,\n",
              "           0.19326726,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.16877575,  1.0319775 , -0.4265387 , -0.7615511 ,  0.21241233,\n",
              "           0.14762469,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.17303447,  1.014245  , -0.43490648, -0.7894402 ,  0.22152895,\n",
              "           0.18233229,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.17729358,  0.9959135 , -0.43490425, -0.81611365,  0.2306455 ,\n",
              "           0.18233128,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.18178138,  0.9784955 , -0.45780784, -0.77557105,  0.23979767,\n",
              "           0.18304358,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.1861763 ,  0.9605211 , -0.44603166, -0.79994965,  0.24645327,\n",
              "           0.13311233,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.19066438,  0.9419023 , -0.45780897, -0.82903665,  0.25561944,\n",
              "           0.18332341,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.19506177,  0.922732  , -0.44629937, -0.85317284,  0.2623133 ,\n",
              "           0.13387753,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.1993804 ,  0.90299577, -0.4363417 , -0.87797654,  0.26689854,\n",
              "           0.09170488,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.20375843,  0.8826273 , -0.44387722, -0.90638196,  0.27311286,\n",
              "           0.12428631,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.2081366 ,  0.86165947, -0.44387594, -0.93305176,  0.27932715,\n",
              "           0.12428591,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.21258025,  0.8410372 , -0.45091385, -0.91781205,  0.2860538 ,\n",
              "           0.13453276,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.21711445,  0.8197662 , -0.46236023, -0.94716245,  0.29526585,\n",
              "           0.18424067,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.22157368,  0.7979214 , -0.45293522, -0.9723155 ,  0.30250564,\n",
              "           0.144796  ,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.2261117 ,  0.7754491 , -0.46279263, -1.0006707 ,  0.3118196 ,\n",
              "           0.18627986,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.23064995,  0.7523779 , -0.46278936, -1.0273442 ,  0.32113355,\n",
              "           0.18627882,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.2353272 ,  0.7301292 , -0.4774057 , -0.9910209 ,  0.3312211 ,\n",
              "           0.20175084,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.24040385,  0.7085273 , -0.5168388 , -0.9622211 ,  0.34081128,\n",
              "           0.19180444,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.24539486,  0.68636584, -0.5060236 , -0.98661083,  0.34807256,\n",
              "           0.14522532,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.25030965,  0.66364235, -0.4964079 , -1.0111352 ,  0.35324746,\n",
              "           0.10349883,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.2553146 ,  0.6402668 , -0.5077976 , -1.0407323 ,  0.36094558,\n",
              "           0.15396218,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.2604878 ,  0.6167871 , -0.52442753, -1.0453485 ,  0.3684552 ,\n",
              "           0.15019314,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.26566115,  0.5927081 , -0.5244251 , -1.0720197 ,  0.3759648 ,\n",
              "           0.15019254,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.27091178,  0.56798214, -0.5341846 , -1.1013631 ,  0.38566363,\n",
              "           0.19397601,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.27616277,  0.5426573 , -0.5341802 , -1.1280372 ,  0.39536238,\n",
              "           0.19397452,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.28133407,  0.5167754 , -0.52411366, -1.1522677 ,  0.40284613,\n",
              "           0.14967468,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.28650555,  0.49029422, -0.5241109 , -1.1789386 ,  0.41032982,\n",
              "           0.1496741 ,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.2919007 ,  0.46384078, -0.54629415, -1.1776904 ,  0.4176366 ,\n",
              "           0.14613555,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.29721898,  0.43683544, -0.53655833, -1.2016443 ,  0.4227455 ,\n",
              "           0.10217804,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.3026209 ,  0.40917572, -0.5471232 , -1.2314206 ,  0.43026578,\n",
              "           0.15040581,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.30817103,  0.3816651 , -0.5622409 , -1.2249224 ,  0.43812594,\n",
              "           0.15720281,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.31378737,  0.35350922, -0.5706042 , -1.2542098 ,  0.44792262,\n",
              "           0.19593352,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.31932038,  0.32481897, -0.5599333 , -1.2772614 ,  0.45520112,\n",
              "           0.14557017,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.32478887,  0.2955848 , -0.5516449 , -1.3008734 ,  0.46048102,\n",
              "           0.10559813,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.33032387,  0.26571488, -0.55997294, -1.3297082 ,  0.4676185 ,\n",
              "           0.14274964,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.33642024,  0.23630764, -0.6153249 , -1.3089168 ,  0.4739147 ,\n",
              "           0.12592462,  0.        ,  0.        ], dtype=float32)],\n",
              "  [1, array([-0.34259433,  0.20623285, -0.625254  , -1.3393639 ,  0.4826322 ,\n",
              "           0.17434931,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.34890833,  0.17621155, -0.6395262 , -1.3371453 ,  0.4917117 ,\n",
              "           0.1815903 ,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.3551422 ,  0.14564553, -0.62938696, -1.3606448 ,  0.4984364 ,\n",
              "           0.13449395,  0.        ,  0.        ], dtype=float32)],\n",
              "  [3, array([-0.36130515,  0.11454013, -0.62030685, -1.3839247 ,  0.5029521 ,\n",
              "           0.09031413,  0.        ,  0.        ], dtype=float32)],\n",
              "  [2, array([-0.36756387,  0.0834308 , -0.63031846, -1.3842789 ,  0.5079651 ,\n",
              "           0.10026032,  0.        ,  0.        ], dtype=float32)],\n",
              "  [0, array([-0.37382275,  0.05172165, -0.630317  , -1.4109476 ,  0.5129781 ,\n",
              "           0.10026016,  0.        ,  1.        ], dtype=float32)],\n",
              "  [2, array([-0.3735307 ,  0.04207851, -0.05043638, -0.4697885 ,  0.5630948 ,\n",
              "           0.9395027 ,  0.        ,  1.        ], dtype=float32)],\n",
              "  [0, array([-0.37361503,  0.0320464 , -0.03489016, -0.45728764,  0.5933303 ,\n",
              "           0.6036111 ,  0.        ,  1.        ], dtype=float32)],\n",
              "  [1, array([-0.37346315,  0.02533501,  0.07199335, -0.2891606 ,  0.52023053,\n",
              "          -1.2508699 ,  0.        ,  1.        ], dtype=float32)],\n",
              "  [3, array([-0.3732184 ,  0.01857792,  0.09715956, -0.2848492 ,  0.4346297 ,\n",
              "          -1.561752  ,  0.        ,  1.        ], dtype=float32)],\n",
              "  [0, array([-0.3729771 ,  0.01143065,  0.10526625, -0.29986694,  0.34551883,\n",
              "          -1.7076727 ,  0.        ,  1.        ], dtype=float32)],\n",
              "  [0, array([-0.3727378 ,  0.00371312,  0.1058992 , -0.3266361 ,  0.26033226,\n",
              "          -1.6900055 ,  0.        ,  1.        ], dtype=float32)],\n",
              "  [2, array([-0.37282413, -0.00357112,  0.07495033, -0.311309  ,  0.17571165,\n",
              "          -1.6924992 ,  0.        ,  1.        ], dtype=float32)],\n",
              "  [3, array([-0.37257853, -0.00965519,  0.1295353 , -0.2615821 ,  0.07115575,\n",
              "          -2.091208  ,  1.        ,  0.        ], dtype=float32)]],\n",
              " 'total_reward': 2.1615999329498834}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYxATeo3aAT6"
      },
      "source": [
        "net = Network(100)\r\n",
        "\r\n",
        "if GPU_ON:\r\n",
        "  net.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP-eG0rbcFzT"
      },
      "source": [
        "\r\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}