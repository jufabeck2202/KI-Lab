{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepReinforcementlearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jufabeck2202/KI-Lab/blob/main/aufgabe6/DeepReinforcementlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwm-vLKt-CxA"
      },
      "source": [
        "General idea:\n",
        "1. Use a neural network that takes as input a state (represented as numbers)\n",
        "and outputs a probability for every action.\n",
        "2. Generate episodes by inputing the current state into the network and\n",
        "sampling actions from the networkâ€™s output. Remember the\n",
        "<state, action> pairs for every episode.\n",
        "3. From these episodes, identify the ones with the highest reward.\n",
        "4. Use the <state, action> pairs from those high reward episodes as\n",
        "training examples for improving the neural network.\n",
        "5. Go back to step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28X7Ou8q-E9e"
      },
      "source": [
        "Task 2\n",
        "1. Define a neural network with two fully connected-layers. The hidden layer uses a\n",
        "Relu activation. The output layer uses a softmax. Try different hidden layer sizes\n",
        "(between 100 and 500). The network takes as input a vector of the current states\n",
        "and gives out a probability for each action.\n",
        "2. Generate 100 episodes by sampling actions using the network output. Limit the\n",
        "number of steps per episode to 500. Sum up the reward of all steps of one episode.\n",
        "3. Print out the mean reward per episode of the 100 episodes.\n",
        "4. Identify the 20 best of those episodes in terms of reward and use the\n",
        "<state, action> pairs of these episodes as training examples for the network.\n",
        "5. Update the weights of the network by performing backpropagation on these <state,\n",
        "action> pairs.\n",
        "6. Repeat steps 2 to 5 until a mean reward of 100 is reached.\n",
        "7. Record a video of the lunar lander by running the trained network on one additional\n",
        "episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQx2xvS-M-i"
      },
      "source": [
        "Hints\n",
        "* Use !pip3 install box2d-py to make the environment work in Colab.\n",
        "* You cannot show the video of your lander in Colab (env.render() fails).  \n",
        "* Workaround: Download the model on your local machine and record the video there,\n",
        "using recording_demo.py as template (see Mattermost).\n",
        "* The loss is not a useful indicator for the learning progress in RL. Instead check how\n",
        "the mean reward develops over time.\n",
        "* The mean reward will jump back and forth quite a bit, but overall should increase.\n",
        "* After roughly 70 training iterations the mean reward should be positive, and after\n",
        "roughly 100 steps be over 100.\n",
        "* Note that these numbers depend on your parameter setting and it may also take\n",
        "longer or shorter.\n",
        "* Reinforcement learning is much more difficult than supervised learning, you have to\n",
        "play around quite a bit to get things into the right direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ozuttP-6-3"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZML6vY495JLW"
      },
      "source": [
        "!pip3 install box2d-py gym > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLOhghCh6375"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import random\n",
        "import torch.optim as optim\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vhGJ8K17AoF",
        "outputId": "7ffbfddb-d276-44a8-f2a1-c6d792ac942a"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "rewards = []\n",
        "\n",
        "##get Action space\n",
        "print(env.observation_space.shape)\n",
        "print(env.action_space)\n",
        "print()\n",
        "print(env.observation_space.sample())\n",
        "print(env.action_space.sample())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "Discrete(4)\n",
            "\n",
            "[ 0.57051045 -1.5992639   0.59137917  0.51901096 -1.4794146   0.02208187\n",
            "  0.24469182  0.5068234 ]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ1t9O_D_YZh"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DA1wvKv7qLe"
      },
      "source": [
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, hidden_size,):\n",
        "        super(Network, self).__init__()\n",
        "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, env.action_space.n)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = f.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_jgm5-hafJE"
      },
      "source": [
        "torch.backends.cudnn.enabled = True\r\n",
        "GPU_ON = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda:0\" if GPU_ON else \"cpu\")\r\n",
        "net = Network(200)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\r\n",
        "if GPU_ON:\r\n",
        "  net.cuda()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pcDn0VkAG4U"
      },
      "source": [
        "# Sample Episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDnOo9WzAGEx"
      },
      "source": [
        "def sample(episodes_n=100, max_steps=5000):\n",
        "  episodes_data = []\n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  for i_episode in range(episodes_n):\n",
        "      steps = []\n",
        "      rewards = []\n",
        "      state = env.reset()\n",
        "      for t in range(max_steps):\n",
        "\n",
        "          state_vector = torch.FloatTensor([state])\n",
        "          probability= softmax(net(state_vector))\n",
        "          selected_action = np.random.choice(len(probability.data.numpy()[0]), p=probability.data.numpy()[0])\n",
        "          new_state, reward, done, info = env.step(selected_action)\n",
        "          rewards.append(reward)\n",
        "          #imporant: use old state\n",
        "          steps.append([selected_action, state])\n",
        "          state = new_state\n",
        "          \n",
        "\n",
        "          if done:\n",
        "              episodes_data.append({ \"mean_reward\": np.array(rewards).sum() ,\"steps\": steps})\n",
        "              break\n",
        "      #print(f\"Mean reward of episode {i_episode}: {np.array(rewards).mean()}\")\n",
        "  return np.array(episodes_data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sERYGp7WirM"
      },
      "source": [
        "t = sample()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucGcZ-es9nXd"
      },
      "source": [
        "def calculate_total_mean_reward(all_episodes):\n",
        "   all_rewards = [d['mean_reward'] for d in all_episodes]\n",
        "   return np.array(all_rewards).mean()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyVPXZ3xVSla"
      },
      "source": [
        "def get_top_20(all_episodes):\r\n",
        "  return heapq.nlargest(20, all_episodes, key=lambda s: s['mean_reward'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0swcXlC9ZGLS"
      },
      "source": [
        "def get_training_data(episode_data):\n",
        "  train_data = []\n",
        "  for entry in episodes_data:\n",
        "    for step in entry['steps']:\n",
        "      one_hot = np.zeros(env.action_space.n)\n",
        "      one_hot[step[0]] = 1\n",
        "      train_data.append([one_hot, step[1]])\n",
        "  return train_data\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S5Objpjkgaa"
      },
      "source": [
        "def get_training_data_batch(episode_data):\n",
        "  x_data = []\n",
        "  y_data = []\n",
        "  for entry in episodes_data:\n",
        "    for step in entry['steps']:\n",
        "      #state\n",
        "      x_data.append(step[1])\n",
        "      #action\n",
        "      y_data.append(step[0])\n",
        "\n",
        "  return [x_data, y_data]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ZUqyjwi0kA"
      },
      "source": [
        "def train(training_data):\n",
        "  for data in training_data:\n",
        "    optimizer.zero_grad()\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    action, observation = data\n",
        "    #convert to torch?\n",
        "    observation=torch.from_numpy(np.expand_dims(observation, axis=0))\n",
        "    action =torch.from_numpy(np.array([np.argmax(action, axis=0)]))\n",
        "    if GPU_ON:\n",
        "      action = action.cuda()\n",
        "      observation = observation.cuda()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    output_action = net(observation)\n",
        "    loss = criterion(output_action, action)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def train_batch(batch_data):\n",
        "  optimizer.zero_grad()\n",
        "  # get the inputs; data is a list of [inputs, labels]\n",
        "  x_data, y_data = batch_data\n",
        "  #convert to torch?\n",
        "  observation=torch.FloatTensor(x_data)\n",
        "  target =torch.LongTensor(y_data)\n",
        "  \n",
        "  if GPU_ON:\n",
        "    target = target.cuda()\n",
        "    observation = observation.cuda()\n",
        "\n",
        "  # forward + backward + optimize\n",
        "  action_pred = net(observation)\n",
        "  #print(action_pred[121])\n",
        "  #print(action_pred[-1])\n",
        "  #print(target[121])\n",
        "  #print(target[-1])\n",
        "  loss = criterion(action_pred, target)\n",
        "  print(f\"Loss {loss}\")\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP-eG0rbcFzT"
      },
      "source": [
        "#train_batch(get_training_data_batch(best_20))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kbmsQ0iurM2"
      },
      "source": [
        "while False:\n",
        "  episodes_data = sample(episodes_n=200)\n",
        "  print(f\"Mean Reward for Sampling 20 episodes {calculate_total_mean_reward(episodes_data)}\")\n",
        "  best_20 = get_top_20(episodes_data)\n",
        "  train(get_training_data(best_20))\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk4n721CILBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f8f52c-db09-4f6c-ef51-112356c9b1ed"
      },
      "source": [
        "for i in range(10000):\n",
        "  episodes_data = sample()\n",
        "  print(f\"{i} Mean Reward for Sampling 20 episodes {calculate_total_mean_reward(episodes_data)}\")\n",
        "  best_20 = get_top_20(episodes_data)\n",
        "  train_batch(get_training_data_batch(best_20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Mean Reward for Sampling 20 episodes -237.00387442808847\n",
            "Loss 1.3673635721206665\n",
            "1 Mean Reward for Sampling 20 episodes -260.7828109804145\n",
            "Loss 1.3259042501449585\n",
            "2 Mean Reward for Sampling 20 episodes -376.7105754603966\n",
            "Loss 1.175499439239502\n",
            "3 Mean Reward for Sampling 20 episodes -427.1510052189414\n",
            "Loss 1.005089282989502\n",
            "4 Mean Reward for Sampling 20 episodes -485.2583841271655\n",
            "Loss 0.8276054263114929\n",
            "5 Mean Reward for Sampling 20 episodes -515.7420076757255\n",
            "Loss 0.5990819334983826\n",
            "6 Mean Reward for Sampling 20 episodes -521.0502960743398\n",
            "Loss 0.45845773816108704\n",
            "7 Mean Reward for Sampling 20 episodes -548.3841626790886\n",
            "Loss 0.3504909574985504\n",
            "8 Mean Reward for Sampling 20 episodes -562.5673020089976\n",
            "Loss 0.2851387560367584\n",
            "9 Mean Reward for Sampling 20 episodes -583.9915371240138\n",
            "Loss 0.23019196093082428\n",
            "10 Mean Reward for Sampling 20 episodes -603.9455182964574\n",
            "Loss 0.1900959312915802\n",
            "11 Mean Reward for Sampling 20 episodes -550.7643337560994\n",
            "Loss 0.17945270240306854\n",
            "12 Mean Reward for Sampling 20 episodes -557.2264914060765\n",
            "Loss 0.15716975927352905\n",
            "13 Mean Reward for Sampling 20 episodes -542.8609949883979\n",
            "Loss 0.17415830492973328\n",
            "14 Mean Reward for Sampling 20 episodes -529.8691182689843\n",
            "Loss 0.18556912243366241\n",
            "15 Mean Reward for Sampling 20 episodes -548.0722855236726\n",
            "Loss 0.1796104460954666\n",
            "16 Mean Reward for Sampling 20 episodes -566.0547700762662\n",
            "Loss 0.1679464876651764\n",
            "17 Mean Reward for Sampling 20 episodes -552.087155768603\n",
            "Loss 0.14456775784492493\n",
            "18 Mean Reward for Sampling 20 episodes -567.3779604473629\n",
            "Loss 0.13622504472732544\n",
            "19 Mean Reward for Sampling 20 episodes -566.9317621069521\n",
            "Loss 0.13891811668872833\n",
            "20 Mean Reward for Sampling 20 episodes -588.0431507645179\n",
            "Loss 0.12406313419342041\n",
            "21 Mean Reward for Sampling 20 episodes -571.2743312530107\n",
            "Loss 0.16240817308425903\n",
            "22 Mean Reward for Sampling 20 episodes -545.103703478217\n",
            "Loss 0.18951192498207092\n",
            "23 Mean Reward for Sampling 20 episodes -570.9657877011371\n",
            "Loss 0.24780705571174622\n",
            "24 Mean Reward for Sampling 20 episodes -529.2394384978638\n",
            "Loss 0.24814549088478088\n",
            "25 Mean Reward for Sampling 20 episodes -589.1253233639516\n",
            "Loss 0.28389137983322144\n",
            "26 Mean Reward for Sampling 20 episodes -541.7747858253459\n",
            "Loss 0.3470163345336914\n",
            "27 Mean Reward for Sampling 20 episodes -545.3182928919829\n",
            "Loss 0.35245460271835327\n",
            "28 Mean Reward for Sampling 20 episodes -595.5639668917817\n",
            "Loss 0.3715384900569916\n",
            "29 Mean Reward for Sampling 20 episodes -569.301689414623\n",
            "Loss 0.359920471906662\n",
            "30 Mean Reward for Sampling 20 episodes -602.741187643745\n",
            "Loss 0.3097110986709595\n",
            "31 Mean Reward for Sampling 20 episodes -588.9337290224713\n",
            "Loss 0.30568939447402954\n",
            "32 Mean Reward for Sampling 20 episodes -633.2531668567572\n",
            "Loss 0.30067116022109985\n",
            "33 Mean Reward for Sampling 20 episodes -628.2149083925091\n",
            "Loss 0.3360454738140106\n",
            "34 Mean Reward for Sampling 20 episodes -591.5134702116742\n",
            "Loss 0.27381688356399536\n",
            "35 Mean Reward for Sampling 20 episodes -622.5039868876844\n",
            "Loss 0.3049318790435791\n",
            "36 Mean Reward for Sampling 20 episodes -616.2021188768832\n",
            "Loss 0.2806786596775055\n",
            "37 Mean Reward for Sampling 20 episodes -628.4886805773942\n",
            "Loss 0.3072543144226074\n",
            "38 Mean Reward for Sampling 20 episodes -630.8858479309197\n",
            "Loss 0.32116541266441345\n",
            "39 Mean Reward for Sampling 20 episodes -644.9065037799625\n",
            "Loss 0.33780258893966675\n",
            "40 Mean Reward for Sampling 20 episodes -607.2589747071074\n",
            "Loss 0.36933040618896484\n",
            "41 Mean Reward for Sampling 20 episodes -610.6708785554928\n",
            "Loss 0.4257764220237732\n",
            "42 Mean Reward for Sampling 20 episodes -625.5579163014656\n",
            "Loss 0.4104130268096924\n",
            "43 Mean Reward for Sampling 20 episodes -604.3377771793373\n",
            "Loss 0.43470075726509094\n",
            "44 Mean Reward for Sampling 20 episodes -619.7966608000369\n",
            "Loss 0.4262925982475281\n",
            "45 Mean Reward for Sampling 20 episodes -626.1912004078724\n",
            "Loss 0.4467552900314331\n",
            "46 Mean Reward for Sampling 20 episodes -637.694448145901\n",
            "Loss 0.4005843698978424\n",
            "47 Mean Reward for Sampling 20 episodes -650.4570710959568\n",
            "Loss 0.42432528734207153\n",
            "48 Mean Reward for Sampling 20 episodes -612.4690837160834\n",
            "Loss 0.38494426012039185\n",
            "49 Mean Reward for Sampling 20 episodes -615.2242615603699\n",
            "Loss 0.3861473798751831\n",
            "50 Mean Reward for Sampling 20 episodes -600.3293255222579\n",
            "Loss 0.3660072088241577\n",
            "51 Mean Reward for Sampling 20 episodes -602.0315819498048\n",
            "Loss 0.3742818236351013\n",
            "52 Mean Reward for Sampling 20 episodes -563.3511574337921\n",
            "Loss 0.385324090719223\n",
            "53 Mean Reward for Sampling 20 episodes -592.9228099673984\n",
            "Loss 0.3849528729915619\n",
            "54 Mean Reward for Sampling 20 episodes -598.906763098948\n",
            "Loss 0.3778502643108368\n",
            "55 Mean Reward for Sampling 20 episodes -624.9332209835366\n",
            "Loss 0.4110386371612549\n",
            "56 Mean Reward for Sampling 20 episodes -589.6952566917438\n",
            "Loss 0.38763999938964844\n",
            "57 Mean Reward for Sampling 20 episodes -587.8550137581399\n",
            "Loss 0.3961085379123688\n",
            "58 Mean Reward for Sampling 20 episodes -591.8434530100603\n",
            "Loss 0.34662458300590515\n",
            "59 Mean Reward for Sampling 20 episodes -576.9651762952378\n",
            "Loss 0.3180057108402252\n",
            "60 Mean Reward for Sampling 20 episodes -569.4288479068254\n",
            "Loss 0.3222900331020355\n",
            "61 Mean Reward for Sampling 20 episodes -582.7681396392406\n",
            "Loss 0.3774254024028778\n",
            "62 Mean Reward for Sampling 20 episodes -588.4512502858847\n",
            "Loss 0.32663190364837646\n",
            "63 Mean Reward for Sampling 20 episodes -560.7829885903825\n",
            "Loss 0.29793038964271545\n",
            "64 Mean Reward for Sampling 20 episodes -575.3065249628462\n",
            "Loss 0.3033502995967865\n",
            "65 Mean Reward for Sampling 20 episodes -566.0328009068562\n",
            "Loss 0.287130206823349\n",
            "66 Mean Reward for Sampling 20 episodes -585.4996454639679\n",
            "Loss 0.2595919072628021\n",
            "67 Mean Reward for Sampling 20 episodes -554.0688967916191\n",
            "Loss 0.2543257772922516\n",
            "68 Mean Reward for Sampling 20 episodes -563.4575262776909\n",
            "Loss 0.30349886417388916\n",
            "69 Mean Reward for Sampling 20 episodes -555.9501963236279\n",
            "Loss 0.33810409903526306\n",
            "70 Mean Reward for Sampling 20 episodes -544.4410598743448\n",
            "Loss 0.26357609033584595\n",
            "71 Mean Reward for Sampling 20 episodes -568.2302550450015\n",
            "Loss 0.27665975689888\n",
            "72 Mean Reward for Sampling 20 episodes -555.9482913904598\n",
            "Loss 0.28350192308425903\n",
            "73 Mean Reward for Sampling 20 episodes -595.0507580850967\n",
            "Loss 0.33353981375694275\n",
            "74 Mean Reward for Sampling 20 episodes -558.8474125639567\n",
            "Loss 0.33437225222587585\n",
            "75 Mean Reward for Sampling 20 episodes -612.2894614597126\n",
            "Loss 0.29478734731674194\n",
            "76 Mean Reward for Sampling 20 episodes -544.4838894502068\n",
            "Loss 0.2677372097969055\n",
            "77 Mean Reward for Sampling 20 episodes -569.0360890908177\n",
            "Loss 0.3299834728240967\n",
            "78 Mean Reward for Sampling 20 episodes -586.7854907744694\n",
            "Loss 0.36602044105529785\n",
            "79 Mean Reward for Sampling 20 episodes -584.6018471770574\n",
            "Loss 0.28989696502685547\n",
            "80 Mean Reward for Sampling 20 episodes -606.6305667625763\n",
            "Loss 0.34441980719566345\n",
            "81 Mean Reward for Sampling 20 episodes -575.2256356575231\n",
            "Loss 0.3270106315612793\n",
            "82 Mean Reward for Sampling 20 episodes -565.7231550813692\n",
            "Loss 0.3112074136734009\n",
            "83 Mean Reward for Sampling 20 episodes -554.0933608898741\n",
            "Loss 0.31728407740592957\n",
            "84 Mean Reward for Sampling 20 episodes -580.519351656962\n",
            "Loss 0.3225843012332916\n",
            "85 Mean Reward for Sampling 20 episodes -564.0177740894032\n",
            "Loss 0.3349488377571106\n",
            "86 Mean Reward for Sampling 20 episodes -580.3560605073733\n",
            "Loss 0.36055251955986023\n",
            "87 Mean Reward for Sampling 20 episodes -591.9728528721218\n",
            "Loss 0.3447229564189911\n",
            "88 Mean Reward for Sampling 20 episodes -596.9422153077882\n",
            "Loss 0.37160035967826843\n",
            "89 Mean Reward for Sampling 20 episodes -558.4805347476321\n",
            "Loss 0.3514764904975891\n",
            "90 Mean Reward for Sampling 20 episodes -586.6063388318212\n",
            "Loss 0.3367091417312622\n",
            "91 Mean Reward for Sampling 20 episodes -593.9796421985218\n",
            "Loss 0.32406753301620483\n",
            "92 Mean Reward for Sampling 20 episodes -562.4389521967112\n",
            "Loss 0.33828821778297424\n",
            "93 Mean Reward for Sampling 20 episodes -551.206015619129\n",
            "Loss 0.3280552625656128\n",
            "94 Mean Reward for Sampling 20 episodes -576.7192262590688\n",
            "Loss 0.3016134202480316\n",
            "95 Mean Reward for Sampling 20 episodes -565.2046794292613\n",
            "Loss 0.3210102319717407\n",
            "96 Mean Reward for Sampling 20 episodes -552.4951095355138\n",
            "Loss 0.3176318109035492\n",
            "97 Mean Reward for Sampling 20 episodes -560.627621707026\n",
            "Loss 0.29793766140937805\n",
            "98 Mean Reward for Sampling 20 episodes -546.9551378700548\n",
            "Loss 0.28931424021720886\n",
            "99 Mean Reward for Sampling 20 episodes -561.9957380561365\n",
            "Loss 0.2874058485031128\n",
            "100 Mean Reward for Sampling 20 episodes -554.844904690246\n",
            "Loss 0.29244333505630493\n",
            "101 Mean Reward for Sampling 20 episodes -560.5666356353261\n",
            "Loss 0.28560778498649597\n",
            "102 Mean Reward for Sampling 20 episodes -564.6387108410886\n",
            "Loss 0.2879132926464081\n",
            "103 Mean Reward for Sampling 20 episodes -527.8301098036519\n",
            "Loss 0.28711655735969543\n",
            "104 Mean Reward for Sampling 20 episodes -569.7850521361689\n",
            "Loss 0.25342243909835815\n",
            "105 Mean Reward for Sampling 20 episodes -578.4163303968568\n",
            "Loss 0.2536732256412506\n",
            "106 Mean Reward for Sampling 20 episodes -547.1635480468175\n",
            "Loss 0.25437426567077637\n",
            "107 Mean Reward for Sampling 20 episodes -586.7156928049327\n",
            "Loss 0.23984409868717194\n",
            "108 Mean Reward for Sampling 20 episodes -580.1774304968299\n",
            "Loss 0.24965612590312958\n",
            "109 Mean Reward for Sampling 20 episodes -550.9175396620296\n",
            "Loss 0.2599729001522064\n",
            "110 Mean Reward for Sampling 20 episodes -621.3930801257916\n",
            "Loss 0.20383180677890778\n",
            "111 Mean Reward for Sampling 20 episodes -602.2036967163945\n",
            "Loss 0.21911673247814178\n",
            "112 Mean Reward for Sampling 20 episodes -629.295160062859\n",
            "Loss 0.17974740266799927\n",
            "113 Mean Reward for Sampling 20 episodes -639.2126920624324\n",
            "Loss 0.1501399576663971\n",
            "114 Mean Reward for Sampling 20 episodes -612.0882075780455\n",
            "Loss 0.17305800318717957\n",
            "115 Mean Reward for Sampling 20 episodes -641.143376691173\n",
            "Loss 0.13727417588233948\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}