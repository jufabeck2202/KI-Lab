{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepReinforcementlearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jufabeck2202/KI-Lab/blob/main/aufgabe6/DeepReinforcementlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwm-vLKt-CxA"
      },
      "source": [
        "General idea:\n",
        "1. Use a neural network that takes as input a state (represented as numbers)\n",
        "and outputs a probability for every action.\n",
        "2. Generate episodes by inputing the current state into the network and\n",
        "sampling actions from the networkâ€™s output. Remember the\n",
        "<state, action> pairs for every episode.\n",
        "3. From these episodes, identify the ones with the highest reward.\n",
        "4. Use the <state, action> pairs from those high reward episodes as\n",
        "training examples for improving the neural network.\n",
        "5. Go back to step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28X7Ou8q-E9e"
      },
      "source": [
        "Task 2\n",
        "1. Define a neural network with two fully connected-layers. The hidden layer uses a\n",
        "Relu activation. The output layer uses a softmax. Try different hidden layer sizes\n",
        "(between 100 and 500). The network takes as input a vector of the current states\n",
        "and gives out a probability for each action.\n",
        "2. Generate 100 episodes by sampling actions using the network output. Limit the\n",
        "number of steps per episode to 500. Sum up the reward of all steps of one episode.\n",
        "3. Print out the mean reward per episode of the 100 episodes.\n",
        "4. Identify the 20 best of those episodes in terms of reward and use the\n",
        "<state, action> pairs of these episodes as training examples for the network.\n",
        "5. Update the weights of the network by performing backpropagation on these <state,\n",
        "action> pairs.\n",
        "6. Repeat steps 2 to 5 until a mean reward of 100 is reached.\n",
        "7. Record a video of the lunar lander by running the trained network on one additional\n",
        "episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQx2xvS-M-i"
      },
      "source": [
        "Hints\n",
        "* Use !pip3 install box2d-py to make the environment work in Colab.\n",
        "* You cannot show the video of your lander in Colab (env.render() fails).  \n",
        "* Workaround: Download the model on your local machine and record the video there,\n",
        "using recording_demo.py as template (see Mattermost).\n",
        "* The loss is not a useful indicator for the learning progress in RL. Instead check how\n",
        "the mean reward develops over time.\n",
        "* The mean reward will jump back and forth quite a bit, but overall should increase.\n",
        "* After roughly 70 training iterations the mean reward should be positive, and after\n",
        "roughly 100 steps be over 100.\n",
        "* Note that these numbers depend on your parameter setting and it may also take\n",
        "longer or shorter.\n",
        "* Reinforcement learning is much more difficult than supervised learning, you have to\n",
        "play around quite a bit to get things into the right direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ozuttP-6-3"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZML6vY495JLW"
      },
      "source": [
        "!pip3 install box2d-py gym > /dev/null"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLOhghCh6375"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import torch.optim as optim\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vhGJ8K17AoF",
        "outputId": "330228f4-4012-435e-8bf9-e1c8e96e80c2"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "rewards = []\n",
        "\n",
        "##get Action space\n",
        "print(env.observation_space.shape)\n",
        "print(env.action_space)\n",
        "print()\n",
        "print(env.observation_space.sample())\n",
        "print(env.action_space.sample())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "Discrete(4)\n",
            "\n",
            "[ 0.04037404  0.9872148  -0.723862   -0.5116671   0.6943135  -0.6465531\n",
            "  0.43555552 -1.3310846 ]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ1t9O_D_YZh"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_jgm5-hafJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e7a512-0dc4-416d-8799-66b4331e573b"
      },
      "source": [
        "torch.backends.cudnn.enabled = True\r\n",
        "GPU_ON = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda:0\" if GPU_ON else \"cpu\")\r\n",
        "device"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DA1wvKv7qLe"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, hidden):\n",
        "        super(Network, self).__init__()\n",
        "        # input == observation space, for this problem its 8\n",
        "        self.linear1 = torch.nn.Linear(env.observation_space.shape[0], hidden)\n",
        "        # action == action space, up down left right for this problem, so 4\n",
        "        self.linear2 = torch.nn.Linear(hidden, env.action_space.n)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hidden = f.relu(self.linear1(state))\n",
        "        return f.softmax(self.linear2(hidden)) "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pcDn0VkAG4U"
      },
      "source": [
        "# Sample Episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDnOo9WzAGEx"
      },
      "source": [
        "def sample(episodes_n=100, max_steps=500):\n",
        "  episodes_data = []\n",
        "  for i_episode in range(episodes_n):\n",
        "      steps = []\n",
        "      rewards = []\n",
        "      observation = env.reset()\n",
        "      for t in range(max_steps):\n",
        "          action = env.action_space.sample()\n",
        "          observation, reward, done, info = env.step(action)\n",
        "          rewards.append(reward)\n",
        "          steps.append([action, observation])\n",
        "          if done:\n",
        "              break\n",
        "      #print(f\"Mean reward of episode {i_episode}: {np.array(rewards).mean()}\")\n",
        "      episodes_data.append({ \"total_reward\": np.array(rewards).sum() ,\"steps\": steps})\n",
        "  return np.array(episodes_data)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sERYGp7WirM"
      },
      "source": [
        "episodes_data = sample()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucGcZ-es9nXd"
      },
      "source": [
        "def calculate_total_mean_reward(episode_data):\n",
        "   all_rewards = [d['total_reward'] for d in episode_data]\n",
        "   return np.array(all_rewards).mean()"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGuoGeK4Dff7",
        "outputId": "71e8ca54-6ca5-4d82-86fc-eeb587a0c504"
      },
      "source": [
        "calculate_total_mean_reward(episodes_data)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-177.20228272692574"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyVPXZ3xVSla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb61ecd0-39ef-404f-ec35-d6cd48e119cf"
      },
      "source": [
        "best_20 = heapq.nlargest(20, episodes_data, key=lambda s: s['total_reward'])\r\n",
        "calculate_total_mean_reward(best_20)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-59.40593139409914"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0swcXlC9ZGLS"
      },
      "source": [
        "def get_training_data(episode_data):\n",
        "  train_data = []\n",
        "  for entry in episodes_data:\n",
        "    for step in entry['steps']:\n",
        "      one_hot = np.zeros(env.action_space.n)\n",
        "      one_hot[step[0]] = 1\n",
        "      train_data.append([one_hot, step[1]])\n",
        "  return train_data\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S5Objpjkgaa"
      },
      "source": [
        "def get_training_data_batch(episode_data):\n",
        "  x_data = []\n",
        "  y_data = []\n",
        "  for entry in episodes_data:\n",
        "    for step in entry['steps']:\n",
        "      one_hot = np.zeros(env.action_space.n)\n",
        "      one_hot[step[0]] = 1\n",
        "      x_data.append(step[1])\n",
        "      y_data.append(one_hot)\n",
        "  return [x_data, y_data]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYxATeo3aAT6"
      },
      "source": [
        "net = Network(100)\r\n",
        "#criterion = nn.CrossEntropyLoss()\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.9)\r\n",
        "if GPU_ON:\r\n",
        "  net.cuda()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGSv6vW7g-HR",
        "outputId": "a05a4e52-199b-43c0-884a-d9e827c32651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output_action2 = net(torch.from_numpy(np.expand_dims(env.observation_space.sample(), axis=0)).float())\n",
        "output_action2 "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1606, 0.2860, 0.2796, 0.2738]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ZUqyjwi0kA"
      },
      "source": [
        "def train(training_data):\n",
        "    for data in training_data:\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        action, observation = data\n",
        "        #convert to torch?\n",
        "        observation=torch.from_numpy(np.expand_dims(observation, axis=0)).float()\n",
        "        action =torch.from_numpy(np.array([np.argmax(action, axis=0)]))\n",
        "        if GPU_ON:\n",
        "          action = action.cuda()\n",
        "          observation = observation.cuda()\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        output_action = net(observation)\n",
        "        print(output_action)\n",
        "        break\n",
        "        loss = criterion(output_action, action)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP-eG0rbcFzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a0c4c8-3bfd-4b7e-9bfe-df17e45c25b6"
      },
      "source": [
        "train(get_training_data(best_20))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2323, 0.2760, 0.2433, 0.2484]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCPC-hT_b4yl",
        "outputId": "4af1411e-48ea-4222-f01b-506cde84f221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "print(f\"Predicted action {input}\")\n",
        "print(f\"Target action{target}\")\n",
        "output = loss(input, target)\n",
        "output.backward()\n",
        "\n",
        "t = np.array([0, 0, 0, 1])\n",
        "np.expand_dims(t, axis=0)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted action tensor([[-1.9234,  0.1754,  1.6204, -0.6128,  1.0844],\n",
            "        [ 0.3016, -0.3686, -0.6444,  0.4330, -0.8447],\n",
            "        [ 0.6179,  1.2917,  0.7774, -1.1160, -1.3543]], requires_grad=True)\n",
            "Target actiontensor([2, 1, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z6JlO2Cdvmq",
        "outputId": "5a93cd39-d7a2-4bfa-9d77-596867417748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "print(input)\n",
        "output = loss(input, target)\n",
        "output.backward()\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.6933,  0.9390,  1.0901, -0.2121,  0.4167],\n",
            "        [-0.6270,  0.4121, -1.8045,  0.6021,  0.2121],\n",
            "        [-0.9574,  1.9374, -1.0427,  1.8245, -0.0944]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quuhyrWwn-Y3"
      },
      "source": [
        "def train2(training_data):\n",
        "  # get the inputs; data is a list of [inputs, labels]\n",
        "  x_data, y_data = training_data\n",
        "  x_data= torch.from_numpy(np.array(x_data)).float()\n",
        "  y_data =torch.from_numpy(np.array(y_data )).float()\n",
        "  if GPU_ON:\n",
        "    x_data = x_data.cuda()\n",
        "    y_data = y_data.cuda()\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "  # forward + backward + optimize\n",
        "  print(f\"single state Value  {x_data[0]}\")\n",
        "  print(f\"single action Value  {y_data[0]}\")\n",
        "  outputs = net(x_data)\n",
        "  print(f\"single predicted action {outputs[0]}\")\n",
        "  loss = criterion(outputs, y_data )\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(loss)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kbmsQ0iurM2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "5d68dc2a-ac1c-456f-c6a7-03b2b7f6061a"
      },
      "source": [
        "while True:\n",
        "  episodes_data = sample()\n",
        "  #calculate_total_mean_reward(episodes_data)\n",
        "  best_20 = heapq.nlargest(20, episodes_data, key=lambda s: s['total_reward'])\n",
        "  train2(get_training_data_batch(best_20))\n",
        "  "
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "single state Value  tensor([ 0.0022,  1.4262,  0.1193,  0.3258, -0.0044, -0.0638,  0.0000,  0.0000])\n",
            "single action Value  tensor([0., 0., 0., 1.])\n",
            "single predicted action tensor([0.2203, 0.2715, 0.2405, 0.2677], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-a97da1b37bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#calculate_total_mean_reward(episodes_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mbest_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_training_data_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-8c290e083c4d>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(training_data)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"single predicted action {outputs[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    }
  ]
}