{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepReinforcementlearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jufabeck2202/KI-Lab/blob/main/aufgabe6/DeepReinforcementlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwm-vLKt-CxA"
      },
      "source": [
        "General idea:\n",
        "1. Use a neural network that takes as input a state (represented as numbers)\n",
        "and outputs a probability for every action.\n",
        "2. Generate episodes by inputing the current state into the network and\n",
        "sampling actions from the networkâ€™s output. Remember the\n",
        "<state, action> pairs for every episode.\n",
        "3. From these episodes, identify the ones with the highest reward.\n",
        "4. Use the <state, action> pairs from those high reward episodes as\n",
        "training examples for improving the neural network.\n",
        "5. Go back to step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28X7Ou8q-E9e"
      },
      "source": [
        "Task 2\n",
        "1. Define a neural network with two fully connected-layers. The hidden layer uses a\n",
        "Relu activation. The output layer uses a softmax. Try different hidden layer sizes\n",
        "(between 100 and 500). The network takes as input a vector of the current states\n",
        "and gives out a probability for each action.\n",
        "2. Generate 100 episodes by sampling actions using the network output. Limit the\n",
        "number of steps per episode to 500. Sum up the reward of all steps of one episode.\n",
        "3. Print out the mean reward per episode of the 100 episodes.\n",
        "4. Identify the 20 best of those episodes in terms of reward and use the\n",
        "<state, action> pairs of these episodes as training examples for the network.\n",
        "5. Update the weights of the network by performing backpropagation on these <state,\n",
        "action> pairs.\n",
        "6. Repeat steps 2 to 5 until a mean reward of 100 is reached.\n",
        "7. Record a video of the lunar lander by running the trained network on one additional\n",
        "episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQx2xvS-M-i"
      },
      "source": [
        "Hints\n",
        "* Use !pip3 install box2d-py to make the environment work in Colab.\n",
        "* You cannot show the video of your lander in Colab (env.render() fails).  \n",
        "* Workaround: Download the model on your local machine and record the video there,\n",
        "using recording_demo.py as template (see Mattermost).\n",
        "* The loss is not a useful indicator for the learning progress in RL. Instead check how\n",
        "the mean reward develops over time.\n",
        "* The mean reward will jump back and forth quite a bit, but overall should increase.\n",
        "* After roughly 70 training iterations the mean reward should be positive, and after\n",
        "roughly 100 steps be over 100.\n",
        "* Note that these numbers depend on your parameter setting and it may also take\n",
        "longer or shorter.\n",
        "* Reinforcement learning is much more difficult than supervised learning, you have to\n",
        "play around quite a bit to get things into the right direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ozuttP-6-3"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZML6vY495JLW"
      },
      "source": [
        "!pip3 install box2d-py gym > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLOhghCh6375"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import random\n",
        "import torch.optim as optim\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vhGJ8K17AoF",
        "outputId": "ec5b4176-5fbd-4da3-c248-eea07b9a6e1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "rewards = []\n",
        "\n",
        "##get Action space\n",
        "print(env.observation_space.shape)\n",
        "print(env.action_space)\n",
        "print()\n",
        "print(env.observation_space.sample())\n",
        "print(env.action_space.sample())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "Discrete(4)\n",
            "\n",
            "[-1.4245257 -0.5118405 -2.3372087 -0.2351139  0.5469467  1.0710894\n",
            "  0.6966683 -1.2581046]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ1t9O_D_YZh"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DA1wvKv7qLe"
      },
      "source": [
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size,):\n",
        "        super(Network, self).__init__()\n",
        "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, env.action_space.n)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_jgm5-hafJE"
      },
      "source": [
        "net = Network(200)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pcDn0VkAG4U"
      },
      "source": [
        "# Sample Episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDnOo9WzAGEx"
      },
      "source": [
        "# get episodes\n",
        "def sample(episodes_n=100, max_steps=5000):\n",
        "  episodes_data = []\n",
        "  \n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  for i_episode in range(episodes_n):\n",
        "      steps = [] # steps[0] is the action, steps[1] is the state\n",
        "      total_reward = 0\n",
        "      state = env.reset()\n",
        "      for i_step in range(max_steps):\n",
        "\n",
        "          state_vector = torch.FloatTensor([state])\n",
        "          probability= softmax(net(state_vector))\n",
        "\n",
        "          selected_action = np.random.choice(len(probability.data.numpy()[0]), p=probability.data.numpy()[0])\n",
        "          new_state, reward, done, info = env.step(selected_action)\n",
        "\n",
        "          rewards.append(reward)\n",
        "          #important: use old state\n",
        "          \n",
        "          steps.append([selected_action, state])\n",
        "          state = new_state\n",
        "          total_reward += reward\n",
        "          \n",
        "\n",
        "          if done:\n",
        "              episodes_data.append({ \"total_reward\": total_reward  ,\"steps\": steps})\n",
        "              break\n",
        "      #print(f\"Mean reward of episode {i_episode}: {np.array(rewards).mean()}\")\n",
        "  return episodes_data\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucGcZ-es9nXd"
      },
      "source": [
        "def calculate_total_mean_reward(all_episodes):\n",
        "   all_rewards = [d['total_reward'] for d in all_episodes]\n",
        "   return np.array(all_rewards).mean()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyVPXZ3xVSla"
      },
      "source": [
        "def get_top_20(all_episodes):\r\n",
        "  return heapq.nlargest(20, all_episodes, key=lambda s: s['total_reward'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw0u6RZw14nU"
      },
      "source": [
        "def filter_batch(all_episodes,percentile=80):\n",
        "\n",
        "    rewards_batch = [d['total_reward'] for d in all_episodes]\n",
        "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
        "   \n",
        "    top_20_p_episodes = []\n",
        "    for episode in all_episodes:\n",
        "      if episode['total_reward'] > reward_threshold:\n",
        "        top_20_p_episodes.append(episode)\n",
        "    return top_20_p_episodes"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S5Objpjkgaa"
      },
      "source": [
        "def get_training_data_batch(data):\n",
        "  observations = []\n",
        "  targets = []\n",
        "  for entry in data:\n",
        "    for step in entry['steps']:\n",
        "      #state\n",
        "      observations.append(step[1])\n",
        "      #action\n",
        "      targets.append(step[0])\n",
        "\n",
        "  return [observations, targets]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ZUqyjwi0kA"
      },
      "source": [
        "def train_batch():\n",
        "  \n",
        "  # get the inputs; data is a list of [inputs, labels]\n",
        "  training_data = sample(episodes_n=100)\n",
        "  best_20 = filter_batch(training_data)\n",
        "  observations, targets = get_training_data_batch(best_20)\n",
        "\n",
        "  # calculate mean for the current episodes:\n",
        "\n",
        "  #convert to torch?\n",
        "  optimizer.zero_grad()\n",
        "  observations=torch.FloatTensor(observations)\n",
        "  targets =torch.LongTensor(targets)\n",
        "  print(targets)\n",
        "\n",
        "  # forward + backward + optimize\n",
        "  action_pred = net(observations)\n",
        "\n",
        "  loss = criterion(action_pred, targets)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  reward_complete = calculate_total_mean_reward(training_data)\n",
        "  print(f\"Mean Reward for Sampling 100 episodes {reward_complete}, Loss: {loss}\")\n",
        "  return reward_complete\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk4n721CILBk",
        "outputId": "e817d68e-9df4-4eb4-e810-5cc25ad7685d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "while True:\n",
        "  if 150 < train_batch():\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Reward for Sampling 100 episodes -196.60228404754926\n",
            "tensor([1, 2, 0,  ..., 3, 3, 0])\n",
            "Loss 1.384766936302185\n",
            "Mean Reward for Sampling 100 episodes -220.61793407841094\n",
            "tensor([3, 2, 3,  ..., 2, 1, 0])\n",
            "Loss 1.3678103685379028\n",
            "Mean Reward for Sampling 100 episodes -166.4845348592457\n",
            "tensor([0, 2, 3,  ..., 2, 0, 3])\n",
            "Loss 1.3699756860733032\n",
            "Mean Reward for Sampling 100 episodes -130.7830805766738\n",
            "tensor([3, 3, 1,  ..., 0, 2, 2])\n",
            "Loss 1.3674547672271729\n",
            "Mean Reward for Sampling 100 episodes -134.91141081156007\n",
            "tensor([0, 1, 1,  ..., 2, 0, 2])\n",
            "Loss 1.36702561378479\n",
            "Mean Reward for Sampling 100 episodes -128.79395277045592\n",
            "tensor([3, 0, 1,  ..., 2, 2, 3])\n",
            "Loss 1.355222463607788\n",
            "Mean Reward for Sampling 100 episodes -129.63727039946968\n",
            "tensor([0, 3, 2,  ..., 0, 1, 1])\n",
            "Loss 1.3329724073410034\n",
            "Mean Reward for Sampling 100 episodes -108.0421215520677\n",
            "tensor([1, 3, 3,  ..., 1, 0, 1])\n",
            "Loss 1.3051036596298218\n",
            "Mean Reward for Sampling 100 episodes -129.3496454057254\n",
            "tensor([2, 2, 3,  ..., 2, 2, 0])\n",
            "Loss 1.268431305885315\n",
            "Mean Reward for Sampling 100 episodes -126.32645695162682\n",
            "tensor([3, 0, 3,  ..., 2, 0, 1])\n",
            "Loss 1.2399976253509521\n",
            "Mean Reward for Sampling 100 episodes -116.30662525611017\n",
            "tensor([1, 1, 2,  ..., 2, 0, 3])\n",
            "Loss 1.2314386367797852\n",
            "Mean Reward for Sampling 100 episodes -154.7321475109388\n",
            "tensor([2, 3, 3,  ..., 2, 2, 2])\n",
            "Loss 1.2200090885162354\n",
            "Mean Reward for Sampling 100 episodes -134.33690282610075\n",
            "tensor([2, 3, 3,  ..., 2, 2, 2])\n",
            "Loss 1.2010596990585327\n",
            "Mean Reward for Sampling 100 episodes -132.27901075345423\n",
            "tensor([1, 2, 2,  ..., 0, 0, 2])\n",
            "Loss 1.215905785560608\n",
            "Mean Reward for Sampling 100 episodes -125.38050939831447\n",
            "tensor([2, 2, 0,  ..., 2, 2, 2])\n",
            "Loss 1.185144066810608\n",
            "Mean Reward for Sampling 100 episodes -112.103393632817\n",
            "tensor([1, 3, 1,  ..., 2, 2, 0])\n",
            "Loss 1.195324420928955\n",
            "Mean Reward for Sampling 100 episodes -97.95613605118538\n",
            "tensor([0, 3, 0,  ..., 2, 1, 3])\n",
            "Loss 1.1927802562713623\n",
            "Mean Reward for Sampling 100 episodes -73.63966822954131\n",
            "tensor([3, 2, 3,  ..., 2, 2, 0])\n",
            "Loss 1.2034777402877808\n",
            "Mean Reward for Sampling 100 episodes -43.09996757461405\n",
            "tensor([1, 3, 1,  ..., 3, 3, 3])\n",
            "Loss 1.239479660987854\n",
            "Mean Reward for Sampling 100 episodes -32.91059992946978\n",
            "tensor([3, 2, 3,  ..., 1, 2, 2])\n",
            "Loss 1.222342848777771\n",
            "Mean Reward for Sampling 100 episodes -21.708326773534104\n",
            "tensor([1, 0, 1,  ..., 1, 2, 2])\n",
            "Loss 1.2219356298446655\n",
            "Mean Reward for Sampling 100 episodes -16.275793867381225\n",
            "tensor([3, 3, 2,  ..., 3, 2, 3])\n",
            "Loss 1.241929292678833\n",
            "Mean Reward for Sampling 100 episodes -16.293769746208337\n",
            "tensor([2, 0, 2,  ..., 2, 2, 1])\n",
            "Loss 1.2045793533325195\n",
            "Mean Reward for Sampling 100 episodes -11.717728449971764\n",
            "tensor([2, 3, 3,  ..., 1, 1, 0])\n",
            "Loss 1.1546489000320435\n",
            "Mean Reward for Sampling 100 episodes -14.535101113187414\n",
            "tensor([1, 2, 1,  ..., 2, 0, 2])\n",
            "Loss 1.2111696004867554\n",
            "Mean Reward for Sampling 100 episodes -0.9690786037848068\n",
            "tensor([0, 1, 3,  ..., 2, 1, 2])\n",
            "Loss 1.243723750114441\n",
            "Mean Reward for Sampling 100 episodes 0.9097565236405337\n",
            "tensor([1, 2, 1,  ..., 3, 3, 1])\n",
            "Loss 1.2106034755706787\n",
            "Mean Reward for Sampling 100 episodes -9.295895461160004\n",
            "tensor([3, 3, 3,  ..., 3, 2, 3])\n",
            "Loss 1.2087559700012207\n",
            "Mean Reward for Sampling 100 episodes -11.346115500544817\n",
            "tensor([0, 3, 2,  ..., 1, 3, 2])\n",
            "Loss 1.1988506317138672\n",
            "Mean Reward for Sampling 100 episodes -8.61619644789661\n",
            "tensor([0, 1, 1,  ..., 2, 2, 2])\n",
            "Loss 1.2097361087799072\n",
            "Mean Reward for Sampling 100 episodes -8.572589465714085\n",
            "tensor([2, 2, 2,  ..., 0, 2, 2])\n",
            "Loss 1.197547197341919\n",
            "Mean Reward for Sampling 100 episodes -4.610413568762509\n",
            "tensor([3, 0, 1,  ..., 1, 0, 1])\n",
            "Loss 1.1884864568710327\n",
            "Mean Reward for Sampling 100 episodes -5.930828897289226\n",
            "tensor([3, 0, 1,  ..., 2, 3, 2])\n",
            "Loss 1.1649266481399536\n",
            "Mean Reward for Sampling 100 episodes 14.74136959155765\n",
            "tensor([2, 3, 2,  ..., 1, 1, 1])\n",
            "Loss 1.1374945640563965\n",
            "Mean Reward for Sampling 100 episodes 14.102647991919996\n",
            "tensor([3, 3, 3,  ..., 1, 2, 0])\n",
            "Loss 1.1958199739456177\n",
            "Mean Reward for Sampling 100 episodes 12.766249600427997\n",
            "tensor([3, 1, 2,  ..., 3, 2, 3])\n",
            "Loss 1.1412999629974365\n",
            "Mean Reward for Sampling 100 episodes 16.463394638901395\n",
            "tensor([1, 3, 3,  ..., 3, 3, 3])\n",
            "Loss 1.0940814018249512\n",
            "Mean Reward for Sampling 100 episodes 19.79459396067589\n",
            "tensor([1, 3, 2,  ..., 3, 3, 3])\n",
            "Loss 1.2203829288482666\n",
            "Mean Reward for Sampling 100 episodes 28.586588368892077\n",
            "tensor([3, 3, 2,  ..., 1, 2, 1])\n",
            "Loss 1.1841291189193726\n",
            "Mean Reward for Sampling 100 episodes 27.81369310006512\n",
            "tensor([3, 3, 0,  ..., 2, 2, 2])\n",
            "Loss 1.2083840370178223\n",
            "Mean Reward for Sampling 100 episodes 24.22697958998616\n",
            "tensor([3, 2, 3,  ..., 2, 1, 1])\n",
            "Loss 1.2172541618347168\n",
            "Mean Reward for Sampling 100 episodes 26.124946241807674\n",
            "tensor([0, 3, 3,  ..., 1, 2, 1])\n",
            "Loss 1.1862961053848267\n",
            "Mean Reward for Sampling 100 episodes 27.486676289843857\n",
            "tensor([3, 3, 3,  ..., 1, 0, 3])\n",
            "Loss 1.1762062311172485\n",
            "Mean Reward for Sampling 100 episodes 29.98868888038893\n",
            "tensor([3, 3, 3,  ..., 1, 1, 1])\n",
            "Loss 1.2250536680221558\n",
            "Mean Reward for Sampling 100 episodes 34.549839282878764\n",
            "tensor([3, 3, 3,  ..., 1, 3, 1])\n",
            "Loss 1.2190672159194946\n",
            "Mean Reward for Sampling 100 episodes 40.137340938283394\n",
            "tensor([3, 3, 3,  ..., 3, 1, 3])\n",
            "Loss 1.205102562904358\n",
            "Mean Reward for Sampling 100 episodes 42.00542428013688\n",
            "tensor([3, 0, 1,  ..., 3, 1, 3])\n",
            "Loss 1.2328294515609741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbdTOUSLbQ_r"
      },
      "source": [
        "# Deprecated code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc6nClmuXcgX"
      },
      "source": [
        "# depricated code\n",
        "def get_training_data(episode_data):\n",
        "  train_data = []\n",
        "  for entry in episodes_data:\n",
        "    for step in entry['steps']:\n",
        "      one_hot = np.zeros(env.action_space.n)\n",
        "      one_hot[step[0]] = 1\n",
        "      train_data.append([one_hot, step[1]])\n",
        "  return train_data\n",
        "\n",
        "def train(training_data):\n",
        "  for data in training_data:\n",
        "    optimizer.zero_grad()\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    action, observation = data\n",
        "    #convert to torch?\n",
        "    observation=torch.from_numpy(np.expand_dims(observation, axis=0))\n",
        "    action =torch.from_numpy(np.array([np.argmax(action, axis=0)]))\n",
        "\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    output_action = net(observation)\n",
        "    loss = criterion(output_action, action)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "while False:\n",
        "  episodes_data = sample(episodes_n=200)\n",
        "  print(f\"Mean Reward for Sampling 20 episodes {calculate_total_mean_reward(episodes_data)}\")\n",
        "  best_20 = get_top_20(episodes_data)\n",
        "  train(get_training_data(best_20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMu2VyYHZItY"
      },
      "source": [
        "# Download Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOYH3BEWdSRq"
      },
      "source": [
        "Code found [here](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb#scrollTo=78BfQoQKOq7z)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78BfQoQKOq7z"
      },
      "source": [
        "\"\"\"!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF92FCzZMWPn"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2499HTRYX2n"
      },
      "source": [
        "## used for downloading videos\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqf4kCnjYktY"
      },
      "source": [
        "env = wrap_env(gym.make(\"LunarLander-v2\"))\n",
        "softmax = nn.Softmax(dim=1)\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    state_vector = torch.FloatTensor([observation])\n",
        "    probability= softmax(net(state_vector))\n",
        "\n",
        "    selected_action = np.random.choice(len(probability.data.numpy()[0]), p=probability.data.numpy()[0])\n",
        "    new_state, reward, done, info = env.step(selected_action)\n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}