{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[KI-Lab2021] Assignment 4 OWCA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLvHDZHBrk0UzZV3ew2pTM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jufabeck2202/KI-Lab/blob/main/%5BKI_Lab2021%5D_Assignment_4_OWCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJKIoo8F5Ijw"
      },
      "source": [
        "Overview:\n",
        "\n",
        "• In this assignment we want to build a model that can generate German nouns\n",
        "given a prefix as input.\n",
        "Example: Input „Jahr\" → Output: „Jahrmarkt“ .\n",
        "\n",
        "• For this purpose, we load German language data and filter it for nouns with spaCy.\n",
        "\n",
        "• We then use a simple RNN to learn to generated names.\n",
        "\n",
        "• In a second step, we switch to a more sophisticated LSTM model and compare the\n",
        "results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1NxUrjP5T12"
      },
      "source": [
        "1. Go to the following Git repo which contains a dataset of German articles:\n",
        "https://github.com/tblock/10kGNAD\n",
        "2. Download the file train.csv\tfrom there and import it into a notebook.\n",
        "3. Drop the label and only use the sentence in each line (the part after the first ; ).\n",
        "4. Use spaCy to get all nouns in the whole file.\n",
        "5. Filter out the nouns that have characters wich are not in all_letters (see\n",
        "code snippet below). The remaining set of nouns constitute our training data.\n",
        "6. Go through the PyTorch tutorial on name generation. Clone the notebook, execute it\n",
        "and understand what is going on.\n",
        "7. Use the same architecture to train a language model on the nouns that were extracted\n",
        "from the German dataset in step 1 to 5. (Note: We do not need a „category“, you have\n",
        "to remove everything that belongs to „category“ from the code).\n",
        "8. Extend the sampling logic in the sample function sucht that:\n",
        "  \n",
        "  i. The model extends any given prefix string by first feeding the prefix into the RNN\n",
        "and then start generating new characters. (Currently the model only samples based\n",
        "on a given starting letter).\n",
        "  \n",
        "  ii. Currently the model uses the highest score during sampling. Change that into\n",
        "drawing a real sample from the softmax distribution.\n",
        "9. Choose 10 random words from the training data and randomly cut-off a prefix. For\n",
        "every of those prefixes give an output for:\n",
        "  1. The model extending the prefix with the max score approach.\n",
        "  2. The mode extending the prefix with a real sampling approach.\n",
        "  3. Randomly pick characters from all_letters to extend the prefix (this is just for\n",
        "comparison reasons, i.e. how would a complete random generation look like).\n",
        "Bonus task (which will be very helpful for the next assignment):\n",
        "• Use an LSTM as described here (but without the embeddings) to train the model.\n",
        "• Feed the whole word at once to the LSTM (i.e. not a single characters at a time).\n",
        "• Generate some words to see the results\n",
        "\n",
        "Do not expect the models to generate always meaningful words. The model should\n",
        "output words that are in some sense similar to valid words and which look much closer\n",
        "to real words than randomly generated output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_igiUR-wtzLi"
      },
      "source": [
        "!pip install -U spacy[cuda100] de &> /dev/null\n",
        "!python -m spacy download de &> /dev/null\n",
        "!pip install -q \"tqdm>=4.36.1\" &> /dev/null"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psGdscU35EmT"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "nlp_ger = spacy.load(\"de\")\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "_5DUqhdlr31Z",
        "outputId": "4705cc0f-2f3a-45f7-e194-e017e95e56d0"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/tblock/10kGNAD/master/articles.csv', names=['temp','Data'], error_bad_lines=False, sep=';')\n",
        "df = df.drop(columns=['temp'])\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Die ARD-Tochter Degeto hat sich verpflichtet, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>App sei nicht so angenommen worden wie geplant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'Zum Welttag der Suizidprävention ist es Zeit,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mitarbeiter überreichten Eigentümervertretern ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Service: Jobwechsel in der Kommunikationsbranc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10268</th>\n",
              "      <td>Die Fundstelle in Südengland ist Unesco-Weltku...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10269</th>\n",
              "      <td>Im Team arbeitet auch ein Inspektor der sudane...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10270</th>\n",
              "      <td>Die zentrale Frage des Projekts: Siedelten Ägy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10271</th>\n",
              "      <td>Klimatische Verschlechterungen dürften zur Auf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10272</th>\n",
              "      <td>Knochen können zum Verständnis des Lebens in d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10273 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Data\n",
              "0      Die ARD-Tochter Degeto hat sich verpflichtet, ...\n",
              "1      App sei nicht so angenommen worden wie geplant...\n",
              "2      'Zum Welttag der Suizidprävention ist es Zeit,...\n",
              "3      Mitarbeiter überreichten Eigentümervertretern ...\n",
              "4      Service: Jobwechsel in der Kommunikationsbranc...\n",
              "...                                                  ...\n",
              "10268  Die Fundstelle in Südengland ist Unesco-Weltku...\n",
              "10269  Im Team arbeitet auch ein Inspektor der sudane...\n",
              "10270  Die zentrale Frage des Projekts: Siedelten Ägy...\n",
              "10271  Klimatische Verschlechterungen dürften zur Auf...\n",
              "10272  Knochen können zum Verständnis des Lebens in d...\n",
              "\n",
              "[10273 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvtWNvTXurU2",
        "outputId": "0d4d5391-09c0-41ec-cc77-dfbc65cb05bf"
      },
      "source": [
        "nouns = []\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0], position=0, leave=True):\n",
        "    # print(token.text, token.pos_, token.dep_, token.head.text)\n",
        "    text = nlp_ger(row.Data)\n",
        "    for word in text:\n",
        "      if word.pos_ == 'NOUN':\n",
        "        nouns.append(word.text)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10273/10273 [05:38<00:00, 30.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq3RJMubzdsd",
        "outputId": "a53ca215-359a-4836-d050-392512196aec"
      },
      "source": [
        "print(len(nouns))\n",
        "nouns = list(dict.fromkeys(nouns))\n",
        "print(f\"Number of German Nouns {len(nouns)}\")\n",
        "all_letters = list(string.ascii_letters + 'ÄÖÜäöüß')\n",
        "nouns = [word for word in nouns if all(ch in all_letters for ch in word)] \n",
        "print(f\"Number of German sanitized Nouns {len(nouns)}\")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102361\n",
            "Number of German Nouns 102361\n",
            "Number of German sanitized Nouns 102361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqEEMOLR1bFH",
        "outputId": "6dab98c0-b8c1-42fe-c72b-202881deddc1"
      },
      "source": [
        "\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of German sanitized Nouns 102361\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}